/* spinlock.hpp
Provides yet another spinlock
(C) 2013-2014 Niall Douglas http://www.nedprod.com/
File Created: Sept 2013


Boost Software License - Version 1.0 - August 17th, 2003

Permission is hereby granted, free of charge, to any person or organization
obtaining a copy of the software and accompanying documentation covered by
this license (the "Software") to use, reproduce, display, distribute,
execute, and transmit the Software, and to prepare derivative works of the
Software, and to permit third-parties to whom the Software is furnished to
do so, all subject to the following:

The copyright notices in the Software and this entire statement, including
the above license grant, this restriction and the following disclaimer,
must be included in all copies of the Software, in whole or in part, and
all derivative works of the Software, unless such copies or derivative
works are solely in the form of machine-executable object code generated by
a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS IN THE SOFTWARE.
*/

#ifndef BOOST_SPINLOCK_HPP
#define BOOST_SPINLOCK_HPP

#include <cassert>
#include <vector>
#include <memory>
#include <array>

#ifdef BOOST_SPINLOCK_ENABLE_VALGRIND
#include "valgrind/drd.h"
#else
#define ANNOTATE_RWLOCK_CREATE(p)
#define ANNOTATE_RWLOCK_DESTROY(p)
#define ANNOTATE_RWLOCK_ACQUIRED(p, s)
#define ANNOTATE_RWLOCK_RELEASED(p, s)
#define ANNOTATE_IGNORE_READS_BEGIN()
#define ANNOTATE_IGNORE_READS_END()
#define ANNOTATE_IGNORE_WRITES_BEGIN()
#define ANNOTATE_IGNORE_WRITES_END()
#define DRD_IGNORE_VAR(x)
#define DRD_STOP_IGNORING_VAR(x)
#ifndef RUNNING_ON_VALGRIND
#define RUNNING_ON_VALGRIND (0)
#endif
#endif

/*! \file spinlock.hpp
\brief Provides boost.spinlock
*/

/*! \mainpage
This is the proposed Boost.Spinlock library, a Boost C++ 11 library providing interesting spinlock related things.
*/

#include "boost/config.hpp"

#include "local-bind-cpp-library/include/import.hpp"
#define BOOST_SPINLOCK_V1 (boost), (spinlock), (v1, inline)
#ifndef BOOST_SPINLOCK_V1_STL11_IMPL
#define BOOST_SPINLOCK_V1_STL11_IMPL std
#endif
#define BOOST_SPINLOCK_V1_NAMESPACE       BOOST_LOCAL_BIND_NAMESPACE      (BOOST_SPINLOCK_V1)
#define BOOST_SPINLOCK_V1_NAMESPACE_BEGIN BOOST_LOCAL_BIND_NAMESPACE_BEGIN(BOOST_SPINLOCK_V1)
#define BOOST_SPINLOCK_V1_NAMESPACE_END   BOOST_LOCAL_BIND_NAMESPACE_END  (BOOST_SPINLOCK_V1)

#define BOOST_STL11_ATOMIC_MAP_NAMESPACE_BEGIN        BOOST_LOCAL_BIND_NAMESPACE_BEGIN(BOOST_SPINLOCK_V1, (stl11, inline))
#define BOOST_STL11_ATOMIC_MAP_NAMESPACE_END          BOOST_LOCAL_BIND_NAMESPACE_END  (BOOST_SPINLOCK_V1, (stl11, inline))
#define BOOST_STL11_CHRONO_MAP_NAMESPACE_BEGIN        BOOST_LOCAL_BIND_NAMESPACE_BEGIN(BOOST_SPINLOCK_V1, (stl11, inline), (chrono))
#define BOOST_STL11_CHRONO_MAP_NAMESPACE_END          BOOST_LOCAL_BIND_NAMESPACE_END  (BOOST_SPINLOCK_V1, (stl11, inline), (chrono))
#define BOOST_STL11_MUTEX_MAP_NAMESPACE_BEGIN         BOOST_LOCAL_BIND_NAMESPACE_BEGIN(BOOST_SPINLOCK_V1, (stl11, inline))
#define BOOST_STL11_MUTEX_MAP_NAMESPACE_END           BOOST_LOCAL_BIND_NAMESPACE_END  (BOOST_SPINLOCK_V1, (stl11, inline))
#define BOOST_STL11_THREAD_MAP_NAMESPACE_BEGIN        BOOST_LOCAL_BIND_NAMESPACE_BEGIN(BOOST_SPINLOCK_V1, (stl11, inline))
#define BOOST_STL11_THREAD_MAP_NAMESPACE_END          BOOST_LOCAL_BIND_NAMESPACE_END  (BOOST_SPINLOCK_V1, (stl11, inline))
#include BOOST_LOCAL_BIND_INCLUDE_STL11(BOOST_SPINLOCK_V1_STL11_IMPL, atomic)
#include BOOST_LOCAL_BIND_INCLUDE_STL11(BOOST_SPINLOCK_V1_STL11_IMPL, chrono)
#include BOOST_LOCAL_BIND_INCLUDE_STL11(BOOST_SPINLOCK_V1_STL11_IMPL, mutex)
#include BOOST_LOCAL_BIND_INCLUDE_STL11(BOOST_SPINLOCK_V1_STL11_IMPL, thread)

// For dump
#include <ostream>


// Turn this on if you have a compiler which understands __transaction_relaxed
//#define BOOST_HAVE_TRANSACTIONAL_MEMORY_COMPILER

BOOST_SPINLOCK_V1_NAMESPACE_BEGIN

    BOOST_LOCAL_BIND_DECLARE(BOOST_SPINLOCK_V1)

    /*! \struct lockable_ptr
     * \brief Lets you use a pointer to memory as a spinlock :)
     */
    template<typename T> struct lockable_ptr : atomic<T *>
    {
      lockable_ptr(T *v=nullptr) : atomic<T *>(v) { }
      //! Returns the memory pointer part of the atomic
      T *get() BOOST_NOEXCEPT_OR_NOTHROW
      {
        union
        {
          T *v;
          size_t n;
        } value;
        value.v=atomic<T *>::load();
        value.n&=~(size_t)1;
        return value.v;
      }
      //! Returns the memory pointer part of the atomic
      const T *get() const BOOST_NOEXCEPT_OR_NOTHROW
      {
        union
        {
          T *v;
          size_t n;
        } value;
        value.v=atomic<T *>::load();
        value.n&=~(size_t)1;
        return value.v;
      }
      T &operator*() BOOST_NOEXCEPT_OR_NOTHROW { return *get(); }
      const T &operator*() const BOOST_NOEXCEPT_OR_NOTHROW { return *get(); }
      T *operator->() BOOST_NOEXCEPT_OR_NOTHROW { return get(); }
      const T *operator->() const BOOST_NOEXCEPT_OR_NOTHROW { return get(); }
    };
    template<typename T> struct spinlockbase
    {
    protected:
      atomic<T> v;
    public:
      typedef T value_type;
      spinlockbase() BOOST_NOEXCEPT_OR_NOTHROW : v(0)
      {
        ANNOTATE_RWLOCK_CREATE(this);
        v.store(0, memory_order_release);
      }
      spinlockbase(const spinlockbase &) = delete;
      //! Atomically move constructs
      spinlockbase(spinlockbase &&o) BOOST_NOEXCEPT_OR_NOTHROW : v(0)
      {
        ANNOTATE_RWLOCK_CREATE(this);
        //v.store(o.v.exchange(0, memory_order_acq_rel));
        v.store(0, memory_order_release);
      }
      ~spinlockbase()
      {
#ifdef BOOST_SPINLOCK_ENABLE_VALGRIND
        if(v.load(memory_order_acquire))
        {
          ANNOTATE_RWLOCK_RELEASED(this, true);
        }
#endif
        ANNOTATE_RWLOCK_DESTROY(this);
      }
      //! Returns the raw atomic
      T load(memory_order o=memory_order_seq_cst) const BOOST_NOEXCEPT_OR_NOTHROW { return v.load(o); }
      //! Sets the raw atomic
      void store(T a, memory_order o=memory_order_seq_cst) BOOST_NOEXCEPT_OR_NOTHROW { v.store(a, o); }
      //! If atomic is zero, sets to 1 and returns true, else false.
      bool try_lock() BOOST_NOEXCEPT_OR_NOTHROW
      {
        if(v.load(memory_order_consume)) // Avoid unnecessary cache line invalidation traffic
          return false;
        T expected=0;
        bool ret=v.compare_exchange_weak(expected, 1, memory_order_acquire, memory_order_consume);
        if(ret)
        {
          ANNOTATE_RWLOCK_ACQUIRED(this, true);
          return true;
        }
        else return false;
      }
      //! If atomic equals expected, sets to 1 and returns true, else false with expected updated to actual value.
      bool try_lock(T &expected) BOOST_NOEXCEPT_OR_NOTHROW
      {
        T t;
        if((t=v.load(memory_order_consume))) // Avoid unnecessary cache line invalidation traffic
        {
          expected=t;
          return false;
        }
        bool ret=v.compare_exchange_weak(expected, 1, memory_order_acquire, memory_order_consume);
        if(ret)
        {
          ANNOTATE_RWLOCK_ACQUIRED(this, true);
          return true;
        }
        else return false;
      }
      //! Sets the atomic to zero
      void unlock() BOOST_NOEXCEPT_OR_NOTHROW
      {
        ANNOTATE_RWLOCK_RELEASED(this, true);
        v.store(0, memory_order_release);
      }
      bool int_yield(size_t) BOOST_NOEXCEPT_OR_NOTHROW { return false; }
    };
    template<typename T> struct spinlockbase<lockable_ptr<T>>
    {
    private:
      lockable_ptr<T> v;
    public:
      typedef T *value_type;
      spinlockbase() BOOST_NOEXCEPT_OR_NOTHROW { }
      spinlockbase(const spinlockbase &) = delete;
      //! Atomically move constructs
      spinlockbase(spinlockbase &&o) BOOST_NOEXCEPT_OR_NOTHROW
      {
        v.store(o.v.exchange(nullptr, memory_order_acq_rel), memory_order_release);
      }
      //! Returns the memory pointer part of the atomic
      T *get() BOOST_NOEXCEPT_OR_NOTHROW { return v.get(); }
      T *operator->() BOOST_NOEXCEPT_OR_NOTHROW { return get(); }
      //! Returns the raw atomic
      T *load(memory_order o=memory_order_seq_cst) BOOST_NOEXCEPT_OR_NOTHROW { return v.load(o); }
      //! Sets the memory pointer part of the atomic preserving lockedness
      void set(T *a) BOOST_NOEXCEPT_OR_NOTHROW
      {
        union
        {
          T *v;
          size_t n;
        } value;
        T *expected;
        do
        {
          value.v=v.load();
          expected=value.v;
          bool locked=value.n&1;
          value.v=a;
          if(locked) value.n|=1;
        } while(!v.compare_exchange_weak(expected, value.v));
      }
      //! Sets the raw atomic
      void store(T *a, memory_order o=memory_order_seq_cst) BOOST_NOEXCEPT_OR_NOTHROW { v.store(a, o); }
      bool try_lock() BOOST_NOEXCEPT_OR_NOTHROW
      {
        union
        {
          T *v;
          size_t n;
        } value;
        value.v=v.load();
        if(value.n&1) // Avoid unnecessary cache line invalidation traffic
          return false;
        T *expected=value.v;
        value.n|=1;
        return v.compare_exchange_weak(expected, value.v);
      }
      void unlock() BOOST_NOEXCEPT_OR_NOTHROW
      {
        union
        {
          T *v;
          size_t n;
        } value;
        value.v=v.load();
        assert(value.n&1);
        value.n&=~(size_t)1;
        v.store(value.v);
      }
      bool int_yield(size_t) BOOST_NOEXCEPT_OR_NOTHROW { return false; }
    };
    //! \brief How many spins to loop, optionally calling the SMT pause instruction on Intel
    template<size_t spins, bool use_pause=true> struct spins_to_loop
    {
      template<class parenttype> struct policy : parenttype
      {
        static BOOST_CONSTEXPR_OR_CONST size_t spins_to_loop=spins;
        policy() {}
        policy(const policy &) = delete;
        policy(policy &&o) BOOST_NOEXCEPT : parenttype(std::move(o)) { }
        bool int_yield(size_t n) BOOST_NOEXCEPT_OR_NOTHROW
        {
          if(parenttype::int_yield(n)) return true;
          if(n>=spins) return false;
          if(use_pause)
          {
#ifdef BOOST_SMT_PAUSE
            BOOST_SMT_PAUSE;
#endif
          }
          return true;
        }
      };
    };
    //! \brief How many spins to yield the current thread's timeslice
    template<size_t spins> struct spins_to_yield
    {
      template<class parenttype> struct policy : parenttype
      {
        static BOOST_CONSTEXPR_OR_CONST size_t spins_to_yield=spins;
        policy() {}
        policy(const policy &) = delete;
        policy(policy &&o) BOOST_NOEXCEPT : parenttype(std::move(o)) { }
        bool int_yield(size_t n) BOOST_NOEXCEPT_OR_NOTHROW
        {
          if(parenttype::int_yield(n)) return true;
          if(n>=spins) return false;
          this_thread::yield();
          return true;
        }
      };
    };
    //! \brief How many spins to sleep the current thread
    struct spins_to_sleep
    {
      template<class parenttype> struct policy : parenttype
      {
        policy() {}
        policy(const policy &) = delete;
        policy(policy &&o) BOOST_NOEXCEPT : parenttype(std::move(o)) { }
        bool int_yield(size_t n) BOOST_NOEXCEPT_OR_NOTHROW
        {
          if(parenttype::int_yield(n)) return true;
          this_thread::sleep_for(chrono::milliseconds(1));
          return true;
        }
      };
    };
    //! \brief A spin policy which does nothing
    struct null_spin_policy
    {
      template<class parenttype> struct policy : parenttype
      {
      };
    };
    template<class T> inline bool is_lockable_locked(T &lockable) BOOST_NOEXCEPT_OR_NOTHROW;
    /*! \class spinlock
    
    Meets the requirements of BasicLockable and Lockable. Also provides a get() and set() for the
    type used for the spin lock.

    So what's wrong with boost/smart_ptr/detail/spinlock.hpp then, and why
    reinvent the wheel?

    1. Non-configurable spin. AFIO needs a bigger spin than smart_ptr provides.

    2. AFIO is C++ 11, and therefore can implement this in pure C++ 11 atomics.

    3. I don't much care for doing writes during the spin. It generates an
    unnecessary amount of cache line invalidation traffic. Better to spin-read
    and only write when the read suggests you might have a chance.
    
    4. This spin lock can use a pointer to memory as the spin lock. See locked_ptr<T>.
    */
    template<typename T, template<class> class spinpolicy2=spins_to_loop<125>::policy, template<class> class spinpolicy3=spins_to_yield<250>::policy, template<class> class spinpolicy4=spins_to_sleep::policy> class spinlock : public spinpolicy4<spinpolicy3<spinpolicy2<spinlockbase<T>>>>
    {
      typedef spinpolicy4<spinpolicy3<spinpolicy2<spinlockbase<T>>>> parenttype;
    public:
      spinlock() { }
      spinlock(const spinlock &) = delete;
      spinlock(spinlock &&o) BOOST_NOEXCEPT : parenttype(std::move(o)) { }
      void lock() BOOST_NOEXCEPT_OR_NOTHROW
      {
        for(size_t n=0;; n++)
        {
          if(parenttype::try_lock())
            return;
          parenttype::int_yield(n);
        }
      }
      //! Locks if the atomic is not the supplied value, else returning false
      bool lock(T only_if_not_this) BOOST_NOEXCEPT_OR_NOTHROW
      {
        for(size_t n=0;; n++)
        {
          T expected=0;
          if(parenttype::try_lock(expected))
            return true;
          if(expected==only_if_not_this)
            return false;
          parenttype::int_yield(n);
        }
      }
    };

    //! \brief Determines if a lockable is locked. Type specialise this for performance if your lockable allows examination.
    template<class T> inline bool is_lockable_locked(T &lockable) BOOST_NOEXCEPT_OR_NOTHROW
    {
      if(lockable.try_lock())
      {
        lockable.unlock();
        return true;
      }
      return false;
    }
    // For when used with a spinlock
    template<class T, template<class> class spinpolicy2, template<class> class spinpolicy3, template<class> class spinpolicy4> inline T is_lockable_locked(spinlock<T, spinpolicy2, spinpolicy3, spinpolicy4> &lockable) BOOST_NOEXCEPT_OR_NOTHROW
    {
#ifdef BOOST_HAVE_TRANSACTIONAL_MEMORY_COMPILER
      // Annoyingly the atomic ops are marked as unsafe for atomic transactions, so ...
      return *((volatile T *) &lockable);
#else
      return lockable.load(memory_order_consume);
#endif
    }
    // For when used with a locked_ptr
    template<class T, template<class> class spinpolicy2, template<class> class spinpolicy3, template<class> class spinpolicy4> inline bool is_lockable_locked(spinlock<lockable_ptr<T>, spinpolicy2, spinpolicy3, spinpolicy4> &lockable) BOOST_NOEXCEPT_OR_NOTHROW
    {
      return ((size_t) lockable.load(memory_order_consume))&1;
    }

#ifndef BOOST_BEGIN_TRANSACT_LOCK
#ifdef BOOST_HAVE_TRANSACTIONAL_MEMORY_COMPILER
#undef BOOST_USING_INTEL_TSX
#define BOOST_BEGIN_TRANSACT_LOCK(lockable) __transaction_relaxed { (void) boost::spinlock::is_lockable_locked(lockable); {
#define BOOST_BEGIN_TRANSACT_LOCK_ONLY_IF_NOT(lockable, only_if_not_this) __transaction_relaxed { if((only_if_not_this)!=boost::spinlock::is_lockable_locked(lockable)) {
#define BOOST_END_TRANSACT_LOCK(lockable) } }
#define BOOST_BEGIN_NESTED_TRANSACT_LOCK(N) __transaction_relaxed
#define BOOST_END_NESTED_TRANSACT_LOCK(N)
#endif // BOOST_BEGIN_TRANSACT_LOCK
#endif

#ifndef BOOST_BEGIN_TRANSACT_LOCK
#define BOOST_BEGIN_TRANSACT_LOCK(lockable) { std::lock_guard<decltype(lockable)> __tsx_transaction(lockable);
#define BOOST_BEGIN_TRANSACT_LOCK_ONLY_IF_NOT(lockable, only_if_not_this) if(lockable.lock(only_if_not_this)) { std::lock_guard<decltype(lockable)> __tsx_transaction(lockable, std::adopt_lock);
#define BOOST_END_TRANSACT_LOCK(lockable) }
#define BOOST_BEGIN_NESTED_TRANSACT_LOCK(N)
#define BOOST_END_NESTED_TRANSACT_LOCK(N)
#endif // BOOST_BEGIN_TRANSACT_LOCK


    /*! \class concurrent_unordered_map
    \brief Provides an unordered_map never slower than std::unordered_map which is thread safe and wait free and optionally no-malloc to use and whose find, insert/emplace
    and erase functions are usually wait free.

    Performance of this STL container, with a recent C++ 11 compiler whose optimiser understands memory ordering [1], is:

    - Within 10% of single threaded performance of a spinlocked std::unordered_map for inserts and removes, and within 33% for finds.
    - For a four core machine with eight threads, performance at full concurrency is 3.6x for inserts and removes, and 3.48x for finds.
    - With Transactional GCC running on Intel TSX capable hardware, performance at full concurrency is 4.07x over single threaded.

    [1]: Currently Microsoft's compiler treats all atomic operations as memory_order_seq_cst irrespective of what they ask for.

    \image html scaling.png
    \image html gcc_vs_vs2013.png

    ## Some notes on this implementation: ##

    - Provides the N3645 (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3645.pdf) low latency no-malloc extensions node_ptr_type, extract() and merge() with
      overload of insert(). Also added is a make_node_ptr() factory function with a rebind/realloc overload, and a make_node_ptrs() batch factory function for those
      with a malloc capable of burst/batch allocation (e.g. Linux, OS X). Finally, merge() is now templatised and rebinds/reallocs node ptrs if necessary.

    - Given the high costs of rehashing (see below), the design sacrifices low load factor performance for high load factor performance. Load factors of 16 or so are not
      significantly slower than load factors of less than 1 if given a well distributed hash function.

    - All operations may operate concurrently with all other operations except rehash() and swap(). If they hit the same bucket they are serialised for obvious reasons. Also
      for obvious reasons a rehash() and swap() must halt all concurrency, these are the only operations which do this.

    - All operations will operate safely in concurrency with all other operations excluding rehash() and swap(), see below. You may get unstable outcomes of course,
      especially if you are inserting and deleting the same key from multiple threads, and no locking is provided per key-value pair, so if you delete it
      from one thread while other threads have references to it you enter undefined behaviour, and probable memory corruption. Strongly consider the use
      of shared_ptr<> as the mapped type if this is a problem for you.
      
    - To very substantially improve concurrency, the following deviations from std::unordered_map<> behaviour have been made:
      - empty() has average complexity O(bucket count/item count/2), worst case O(bucket count) when the map is empty.
      - size() always has complexity O(bucket count). If you do rehash(size()) to make load factor to 1.0, remember this can become very slow for large
        numbers of items. The map is deliberately more tolerant than most to collisions, it can cope with load factors of 8.0 or so without much slowdown.
      - emplace() consumes its rvalue referenced items even when an exception is thrown i.e. the "has no effect" rule is violated. The map itself is untouched
        however. Chances are real world code will never notice this, but if you do, insert() correctly does not consume arguments if exceptions are thrown.

    - clear() is safe concurrent with all other operations with the obvious caveat that any item you are using may be asynchronously deleted. Note that inserting
      items concurrently to a clear() has a possibility of failing to clear some of those newly inserted items. erase(key) is safe however.
      
    - merge() is safe concurrent with all other operations with the obvious caveat that any iterators inside the map being merged out of will invalidate.
      Iterators inside the map being merged into are safe. Note that inserting items into the merging map concurrently has a possibility of failing to merge
      some of those newly inserted items.

    ## Awaiting implementation ##
    
    - Not everything is fully implemented and is marked as FIXME hopefully for some future GSoC student. In particular:

      - C++ 03 support
      - const iterators are typedefed to be normal iterators as const iterators haven't been implemented yet. I explicitly left this for GSoC students to
        do as part of their entrance exam.
      - All const member functions are const_casted to their non-const forms
      - Local iterators
      - Copy and move construction plus copy and move assignment

    ## swap() thread safety ##
    
    swap() is _probably_ safe to use concurrent to all other operations. I say probably because it swaps the hasher, key comparer, max_load_factor and
    min_bucket_capacity individually and without serialisation. If any of those race state, you are in trouble, however for 99% of uses of this class
    this won't occur because the default hasher and key comparer carry no state, and max_load_factor and min_bucket_capacity are only ever used by
    rehash(), which cannot happen during a swap.
    
    swap() is serialised with respect to itself and rehash(), so you are safe to swap and rehash concurrently.
    
    ## rehash() thread safety ##
    
    To help you make sane concurrent use of the map, insert/emplace and erase never invalidate iterators nor references. This implies that rehashing is
    manual. If you perform a manual rehash which will invalidate all iterators, you must manually call it at a time when the following conditions are true:
      
      1. All iterators are not in use in other threads. That means calls such as erase(find(value)) cannot be happening. Use erase(value) instead.
      2. No call which consumes iterators is happening in other threads.
      3. All calls which return iterators must never use that returned iterator, and must throw it away without using it. Use at() or operator[] if you
         need to safely access a found value concurrent to a rehash.
      4. All the following calls which use iterators internally are not happening in other threads:
         - count()
         - merge() where input container is not a concurrent_unordered_map.

    There is more bad news unfortunately. rehash() is concurrent safe if and only if you **YOU DO NOT CALL REHASH TOO FREQUENTLY**.
    This is because rehash() is made safe concurrent by keeping around many old bucket lists but marked to tell threads using them to reload the bucket list,
    this allows all other operations to skip locking the buckets and vastly improves performance.
    Therefore if you call rehash() many times in quick succession and push a bucket list still in use off the end of the ring buffer which is currently
    eight buckets long, you end up deleting that bucket list possibly before threads have had a chance to notice that they must reload the bucket list,
    and you will have a race and probable memory corruption.
      
    As much as this might sound terrible because this concurrent_unordered_map implementation cannot GUARANTEE thread safety, such are the costs of
    rehashing on concurrency that anyone using this class will only rarely rehash, especially as size() is slow. We suggest that you run a rehash
    no more than once per second
    (putting rehash on a timer is an excellent idea), the unit test suite tests 1000 rehashes per second and sees no segfaults [1], so a margin of one thousand
    should be sufficient for production code. If you are super paranoid, never rehash more than once per minute, the chances of a thread taking
    that long between taking a copy of the location of the bucket list and examining its bucket lock are extraordinarily remote.
    
    [1]: With 1000 rehashes per second, valgrind spots just two reads of uninitialised data out of 800m inserts and deletes and ?k rehashes. Which
    is quite rare.

    ## Race detecting tools ##

    The ThreadSanitizer runs at a reasonable speed, and it's what the automated unit testing CI uses.

    valgrind helgrind runs so slowly as to be pretty much useless. valgrind drd is much better, but still very slow. Define BOOST_SPINLOCK_ENABLE_VALGRIND
    to build in valgrind race detection instrumentation.
    
    ## Acknowledgements: ##
    I'd like to thank Howard Hinnant and Jonathan Wakely for helping me embrace and extend N3645.
    
    I'd like to thank Hans Boehm for working through the atomic memory order semantics I used and
    telling me where I could relax things.
    */
    template<class Key, class T, class Hash=std::hash<Key>, class Pred=std::equal_to<Key>, class Alloc=std::allocator<std::pair<const Key, T>>> class concurrent_unordered_map
    {
    public:
      typedef Key key_type;
      typedef T mapped_type;
      typedef std::pair<const key_type, mapped_type> value_type;
      typedef Hash hasher;
      typedef Pred key_equal;
      typedef Alloc allocator_type;

      typedef value_type& reference;
      typedef const value_type& const_reference;
      typedef value_type* pointer;
      typedef const value_type *const_pointer;
      typedef std::size_t size_type;
      typedef std::ptrdiff_t difference_type;
    private:
      spinlock<bool> _rehash_lock;
      hasher _hasher;
      key_equal _key_equal;
      allocator_type _allocator;
      float _max_load_factor;
      size_t _min_bucket_capacity;
      template<class U> struct undoer_t
      {
        U callable;
        bool dismissed;
        undoer_t(U c) : callable(std::move(c)), dismissed(false) { }
        undoer_t(const undoer_t &) = delete;
        undoer_t(undoer_t &&o) : callable(std::move(o.callable)), dismissed(o.dismissed) { o.dismissed=true; }
        ~undoer_t() { if(!dismissed) callable(); }
      };
      template<class U> undoer_t<U> undoer(U c) { return undoer_t<U>(std::move(c)); }
    public:
      //! The N3645 extension of a unique_ptr like smart pointer to an allocated value_type
      class node_ptr_type
      {
        friend class concurrent_unordered_map;
        typedef concurrent_unordered_map container;
      public:
        typedef typename container::value_type value_type;
        typedef typename container::allocator_type allocator_type;
        typedef typename container::reference reference;
        typedef typename container::pointer pointer;
      private:
        typedef value_type container_node_type;
        allocator_type allocator;
        value_type *p;
        node_ptr_type(allocator_type &_allocator) : allocator(_allocator), p(nullptr) { }
        template<class... Args> node_ptr_type(allocator_type &_allocator, Args &&... args)
          : allocator(_allocator), p(nullptr)
        {
          p=allocator.allocate(1);
          try
          {
            allocator.construct(p, std::forward<Args>(args)...);
          }
          catch(...)
          {
            allocator.deallocate(p, 1);
            throw;
          }
        }
      public:
        BOOST_CONSTEXPR node_ptr_type() BOOST_NOEXCEPT : p(nullptr) {}
        BOOST_CONSTEXPR node_ptr_type(std::nullptr_t) BOOST_NOEXCEPT : p(nullptr) {}
        node_ptr_type(node_ptr_type &&o) BOOST_NOEXCEPT : allocator(std::move(o.allocator)), p(o.p) { o.p=nullptr; }
        node_ptr_type(const node_ptr_type &)=delete;
        node_ptr_type &operator=(node_ptr_type &&o) BOOST_NOEXCEPT_IF(std::is_nothrow_destructible<value_type>::value)
        {
          this->~node_ptr_type();
          new(this) node_ptr_type(std::move(o));
          return *this;
        }
        node_ptr_type &operator=(std::nullptr_t) BOOST_NOEXCEPT_IF(std::is_nothrow_destructible<value_type>::value)
        {
          reset();
          return *this;
        }
        node_ptr_type &operator=(const node_ptr_type &o)=delete;
        ~node_ptr_type() BOOST_NOEXCEPT_IF(std::is_nothrow_destructible<value_type>::value)
        {
          reset();
        }
        pointer get() const BOOST_NOEXCEPT { return p; }
        reference operator*() BOOST_NOEXCEPT { return *p; }
        pointer operator->() BOOST_NOEXCEPT { return p; }
        allocator_type get_allocator() const BOOST_NOEXCEPT { return allocator; }
        explicit operator bool() const BOOST_NOEXCEPT { return p!=nullptr; }
        pointer release() BOOST_NOEXCEPT
        {
          value_type *ret=p;
          p=nullptr;
          return ret;
        }
        void reset() BOOST_NOEXCEPT_IF(std::is_nothrow_destructible<value_type>::value)
        {
          if(p)
          {
            allocator.destroy(p);
            allocator.deallocate(p, 1);
            p=nullptr;
          }
        }
        void reset(std::nullptr_t) BOOST_NOEXCEPT_IF(std::is_nothrow_destructible<value_type>::value) { reset(); }
        void swap(node_ptr_type &o) BOOST_NOEXCEPT
        {
          node_ptr_type temp(std::move(*this));
          *this=std::move(o);
          o=std::move(temp);
        }
      };
    private:
      struct item_type
      {
        value_type *p;
        size_t hash;
        item_type() BOOST_NOEXCEPT : p(nullptr), hash(0) { }
        item_type(size_t _hash, value_type *_p) BOOST_NOEXCEPT : p(_p), hash(_hash) { }
        item_type(size_t _hash, node_ptr_type &&_p) BOOST_NOEXCEPT : p(_p.release()), hash(_hash) { }
        item_type(item_type &&o) BOOST_NOEXCEPT : p(std::move(o.p)), hash(o.hash) { o.p=nullptr; o.hash=0; }
        item_type(const item_type &o) = delete;
        ~item_type() BOOST_NOEXCEPT
        {
          assert(!p);
        }
      };
      typedef typename allocator_type::template rebind<item_type>::other item_type_allocator;
      struct bucket_type_impl
      {
        spinlock<unsigned char> lock;  // = 2 if you need to reload the bucket list
        atomic<unsigned> count; // count is used items in there
        std::vector<item_type, item_type_allocator> items;
        bucket_type_impl() : count(0), items(0) { DRD_IGNORE_VAR(count); count.store(0, memory_order_release); }
        ~bucket_type_impl() BOOST_NOEXCEPT_IF(std::is_nothrow_destructible<decltype(items)>::value) { DRD_STOP_IGNORING_VAR(count); }
        bucket_type_impl(bucket_type_impl &&) BOOST_NOEXCEPT : count(0) { DRD_IGNORE_VAR(count); count.store(0, memory_order_release); }
        bucket_type_impl(const bucket_type_impl &) = delete;
      };
#if 1 // improves concurrent write performance
      struct bucket_type : bucket_type_impl
      {
        char pad[64-sizeof(bucket_type_impl)];
        bucket_type()
        {
          static_assert(sizeof(bucket_type)==64, "bucket_type is not 64 bytes long!");
        }
        bucket_type(bucket_type &&o) BOOST_NOEXCEPT : bucket_type_impl(std::move(o)) { }
        bucket_type(const bucket_type &) = delete;
      };
#else
      typedef bucket_type_impl bucket_type;
#endif
      typedef typename allocator_type::template rebind<bucket_type>::other bucket_type_allocator;
      typedef std::vector<bucket_type, bucket_type_allocator> buckets_type;
      std::atomic<buckets_type *> _buckets;
      typedef std::array<buckets_type *, 8> old_buckets_type;
      old_buckets_type _oldbuckets;
      typename old_buckets_type::iterator _oldbucketit;
      typename buckets_type::iterator _get_bucket(size_t k) BOOST_NOEXCEPT
      {
        //k ^= k + 0x9e3779b9 + (k<<6) + (k>>2); // really need to avoid sequential keys tapping the same cache line
        //k ^= k + 0x9e3779b9; // really need to avoid sequential keys tapping the same cache line
        buckets_type &buckets=*_buckets.load(memory_order_consume);
        ANNOTATE_IGNORE_READS_BEGIN(); // doesn't realise that buckets never changes, so lack of lock between write and read not important
        size_type i=k % buckets.size();
        typename buckets_type::iterator ret=buckets.begin()+i;
        ANNOTATE_IGNORE_READS_END();
        return ret;
      }
      static float _calc_max_load_factor() BOOST_NOEXCEPT
      {
        return 1.0f;
#if 0
        // We are intentionally very tolerant to load factor, so set to
        // however many item_type's fit into 128 bytes
        float ret=128/sizeof(item_type);
        if(ret<1) ret=0;
        return ret;
#endif
      }
    public:
      //! The iterator. Will call abort() if you try to dereference it after its map has been rehashed.
      class iterator : public std::iterator<std::forward_iterator_tag, value_type, difference_type, pointer, reference>
      {
        concurrent_unordered_map *_parent;
        buckets_type *_bucket_data; // used for sanity check that he hasn't rehashed
        typename buckets_type::iterator _itb;
        size_t _offset, _pending_incr; // used to avoid erase() doing a costly increment unless necessary
        friend class concurrent_unordered_map;
        static typename buckets_type::iterator dead() { static typename buckets_type::iterator it; return it; }
        iterator(const concurrent_unordered_map *parent) BOOST_NOEXCEPT : _parent(const_cast<concurrent_unordered_map *>(parent)), _bucket_data(parent->_buckets.load(memory_order_consume)), _offset((size_t) -1), _pending_incr(1) { buckets_type &buckets=*_bucket_data; _itb=buckets.begin(); }
        iterator(const concurrent_unordered_map *parent, std::nullptr_t) BOOST_NOEXCEPT : _parent(const_cast<concurrent_unordered_map *>(parent)), _bucket_data(parent->_buckets.load(memory_order_consume)), _itb(dead()), _offset((size_t) -1), _pending_incr(0) { }
        void _catch_up() BOOST_NOEXCEPT
        {
          assert(_itb!=dead());
          if(_itb==dead())
            abort();
          buckets_type &buckets=*_parent->_buckets.load(memory_order_consume);
          while(_pending_incr && _itb!=buckets.end())
          {
            // Check for staleness
            assert(_bucket_data==_parent->_buckets.load(memory_order_consume));
            if(_bucket_data!=_parent->_buckets.load(memory_order_consume))
              abort(); // stale iterator
            bucket_type &b=*_itb;
            BOOST_BEGIN_TRANSACT_LOCK_ONLY_IF_NOT(b.lock, 2)
            {
              auto &items=b.items;
              _offset++;
              for(item_type *i=items.data()+_offset, *e=items.data()+items.size(); i<e; _offset++, i++)
                if(i->p)
                  if(!(--_pending_incr)) break;
            }
            BOOST_END_TRANSACT_LOCK(b.lock)
            if(_pending_incr)
            {
              do
              {
                ++_itb;
                _offset=(size_t) -1;
              } while(_itb!=buckets.end() && !_itb->count.load(memory_order_consume));
            }
          }
          if(_itb==buckets.end())
            _itb=dead();
        }
        void _catch_up() const BOOST_NOEXCEPT { const_cast<iterator *>(this)->_catch_up(); }
      public:
        iterator() BOOST_NOEXCEPT : _parent(nullptr), _bucket_data(nullptr), _itb(dead()), _offset((size_t) -1), _pending_incr(0) { }
        bool operator!=(const iterator &o) const BOOST_NOEXCEPT { _catch_up(); return _itb!=o._itb || _offset!=o._offset; }
        bool operator==(const iterator &o) const BOOST_NOEXCEPT { _catch_up(); return _itb==o._itb && _offset==o._offset; }
        iterator &operator++() BOOST_NOEXCEPT
        {
          if(_itb==dead())
            return *this;
          ++_pending_incr;
          return *this;
        }
        iterator operator++(int) BOOST_NOEXCEPT { iterator t(*this); operator++(); return t; }
        value_type &operator*() BOOST_NOEXCEPT { _catch_up(); return *_itb->items[_offset].p; }
        value_type *operator->() BOOST_NOEXCEPT { _catch_up(); return _itb->items[_offset].p; }
      };
      typedef iterator const_iterator; // FIXME
      // local_iterator
      // const_local_iterator
      concurrent_unordered_map() : concurrent_unordered_map(0) { }
      explicit concurrent_unordered_map(size_type n, const hasher &h=hasher(), const key_equal &ke=key_equal(), const allocator_type &al=allocator_type()) : _hasher(h), _key_equal(ke), _allocator(al), _max_load_factor(_calc_max_load_factor()), _min_bucket_capacity(0), _buckets(new buckets_type(n>0 ? n : 13)), _oldbucketit(_oldbuckets.begin()) { _oldbuckets.fill(nullptr);}
      explicit concurrent_unordered_map(const allocator_type &al) : concurrent_unordered_map(0, hasher(), key_equal(), al) { }
      concurrent_unordered_map(size_type n, const allocator_type &al) : concurrent_unordered_map(n, hasher(), key_equal(), al) { }
      concurrent_unordered_map(size_type n, const hasher &h, const allocator_type &al) : concurrent_unordered_map(n, h, key_equal(), al) { }

      template<class InputIterator> concurrent_unordered_map(InputIterator first, InputIterator last, size_type n=0, const hasher &h=hasher(), const key_equal &ke=key_equal(), const allocator_type &al=allocator_type()) : concurrent_unordered_map(n, h, ke, al) { insert(first, last); }
      template<class InputIterator> concurrent_unordered_map(InputIterator first, InputIterator last, size_type n, const allocator_type &al) : concurrent_unordered_map(n, al) { insert(first, last); }
      template<class InputIterator> concurrent_unordered_map(InputIterator first, InputIterator last, size_type n, const hasher &h, const allocator_type &al) : concurrent_unordered_map(n, h, al) { insert(first, last); }
      
      concurrent_unordered_map(std::initializer_list<value_type> il, size_type n=0, const hasher &h=hasher(), const key_equal &ke=key_equal(), const allocator_type &al=allocator_type()) : concurrent_unordered_map(n, h, ke, al) { insert(std::move(il)); }
      concurrent_unordered_map(std::initializer_list<value_type> il, size_type n, const allocator_type &al) : concurrent_unordered_map(n, al) { insert(std::move(il)); }
      concurrent_unordered_map(std::initializer_list<value_type> il, size_type n, const hasher &h, const allocator_type &al) : concurrent_unordered_map(n, h, al) { insert(std::move(il)); }
      ~concurrent_unordered_map() BOOST_NOEXCEPT_IF(std::is_nothrow_destructible<decltype(_buckets)>::value)
      {
        buckets_type &buckets=*_buckets.load(memory_order_consume);
        // Raise the rehash lock and leave it raised
        _rehash_lock.lock();
        // Lock all existing buckets
        for(auto &b : buckets)
          b.lock.lock();
        for(auto &b : buckets)
        {
          for(auto &i : b.items)
          {
            node_ptr_type former(_allocator);
            former.p=i.p;
            i.p=nullptr;
          }
          b.items.clear();
          b.count.store(0, memory_order_release);
        }
        buckets.clear();
        delete _buckets.exchange(nullptr, memory_order_acq_rel);
        for(auto &i : _oldbuckets)
        {
          delete i;
          i=nullptr;
        }
      }
    private:
      // FIXME Awaiting implementation
      concurrent_unordered_map(const concurrent_unordered_map &);
      concurrent_unordered_map(concurrent_unordered_map &&) BOOST_NOEXCEPT;
      concurrent_unordered_map &operator=(const concurrent_unordered_map &);
      concurrent_unordered_map &operator=(concurrent_unordered_map &&) BOOST_NOEXCEPT;
    public:
      //! Slow call returning if a map is empty. Average time is O(bucket count/item count/2)
      bool empty() const BOOST_NOEXCEPT
      {
        bool done;
        do
        {
          buckets_type &buckets=*_buckets.load(memory_order_consume);
          done=true;
          for(auto &b : buckets)
          {
            // If the lock is other state we need to reload bucket list
            if(b.lock.load(memory_order_relaxed)==2)
            {
              done=false;
              break;
            }
            if(b.count.load(memory_order_relaxed))
              return false;
          }
        } while(!done);
        return true;
      }
      //! Even slower call returning the number of items in the map. Average time is O(bucket count)
      size_type size() const BOOST_NOEXCEPT
      {
        size_type ret;
        bool done;
        do
        {
          buckets_type &buckets=*_buckets.load(memory_order_consume);
          done=true;
          ret=0;
          for(auto &b : buckets)
          {
            // If the lock is other state we need to reload bucket list
            if(b.lock.load(memory_order_relaxed)==2)
            {
              done=false;
              break;
            }
            ret+=b.count.load(memory_order_relaxed);
          }
        } while(!done);
        return ret;
      }
      size_type max_size() const BOOST_NOEXCEPT { return (size_type) -1; }
      iterator begin() BOOST_NOEXCEPT { return iterator(this); }
      const_iterator begin() const BOOST_NOEXCEPT { return const_iterator(this); }
      const_iterator cbegin() const BOOST_NOEXCEPT { return const_iterator(this); }
      iterator end() BOOST_NOEXCEPT { return iterator(this, nullptr); }
      const_iterator end() const BOOST_NOEXCEPT { return const_iterator(this, nullptr); }
      const_iterator cend() const BOOST_NOEXCEPT { return const_iterator(this, nullptr); }
    private:
      template<class C> void _find(const key_type &k, C &&c) BOOST_NOEXCEPT
      {
        size_t h=_hasher(k);
        bool done=false;
        do
        {
          auto itb=_get_bucket(h);
          bucket_type &b=*itb;
          size_t offset=0;
          if(!b.count.load(memory_order_relaxed))
            done=true;
          else
          {
            // Should run completely concurrently with other finds
            BOOST_BEGIN_TRANSACT_LOCK_ONLY_IF_NOT(b.lock, 2)
            {
              auto &items=b.items;
              for(item_type *i=items.data()+offset, *e=items.data()+items.size();;)
              {
                for(; i<e && i->hash!=h; offset++, i++);
                if(i==e)
                  break;
                if(i->p && _key_equal(k, i->p->first))
                {
                  if((done=c(itb, offset, i)))
                    break;
                }
                else offset++, i++;
              }
              done=true;
            }
            BOOST_END_TRANSACT_LOCK(b.lock)
          }
        } while(!done);
      }
    public:
      iterator find(const key_type &k) BOOST_NOEXCEPT
      {
        iterator ret=end();
        _find(k, [&](typename buckets_type::iterator &itb, size_t offset, item_type *i){
          ret._itb=itb;
          ret._offset=offset;
          return true;
        });
        return ret;
      }
      const_iterator find(const key_type &k) const BOOST_NOEXCEPT { return const_cast<concurrent_unordered_map *>(this)->find(k); } // FIXME
      //! Rehash safe way of concurrently accessing a mapped type
      mapped_type &at(const key_type &k)
      {
        mapped_type *ret=nullptr;
        _find(k, [&](typename buckets_type::iterator &itb, size_t offset, item_type *i){
          ret=&i->p->second;
          return true;
        });
        if(!ret) throw std::out_of_range("Key not found");
        return *ret;
      }
      //! Rehash safe way of concurrently accessing a mapped type
      const mapped_type &at(const key_type &k) const { return const_cast<concurrent_unordered_map *>(this)->at(k); } // FIXME
      //! Rehash safe way of concurrently creating and accessing a mapped type
      mapped_type &operator[](const key_type &k)
      {
        mapped_type *ret=nullptr;
        do
        {
          _find(k, [&](typename buckets_type::iterator &itb, size_t offset, item_type *i){
            ret=&i->p->second;
            return true;
          });
          if(!ret)
            emplace(k, mapped_type());
        } while(!ret);
        return *ret;
      }
      //! Rehash safe way of concurrently creating and accessing a mapped type
      mapped_type &operator[](key_type &&k)
      {
        mapped_type *ret=nullptr;
        node_ptr_type e=make_node_ptr(value_type(std::move(k), mapped_type()));
        try
        {
          do
          {
            _find(e->first, [&](typename buckets_type::iterator &itb, size_t offset, item_type *i){
              ret=&i->p->second;
              return true;
            });
            if(!ret)
            {
              mapped_type *t=&e->second;
              insert(std::move(e));
              ret=t;
            }
          } while(!ret);
        }
        catch(...)
        {
          k=std::move(e->first);
          throw;
        }
        return *ret;
      }
      //! NOT rehash safe
      size_type count(const key_type &k) const BOOST_NOEXCEPT { return end()!=find(k) ? 1 : 0; }
      std::pair<iterator, iterator> equal_range(const key_type &k) BOOST_NOEXCEPT
      {
        iterator it=find(k);
        return std::make_pair(it, it);
      }
      std::pair<const_iterator, const_iterator> equal_range(const key_type &k) const BOOST_NOEXCEPT
      {
        const_iterator it=find(k);
        return std::make_pair(it, it);
      }

      //! \brief Factory function for a node_ptr_type.
      template<class... Args> node_ptr_type make_node_ptr(Args&&... args)
      {
        return node_ptr_type(_allocator, std::forward<Args>(args)...);
      }
      /*! \brief Lets you rebind a node_ptr_type from another map into this map. If allocators are
      dissimilar or the supplied type is not moveable, performs a new allocation using the container allocator.
      */
      template<class U> node_ptr_type rebind_node_ptr(typename U::node_ptr_type &&p)
        BOOST_NOEXCEPT_IF(!(!std::is_same<Alloc, typename U::node_ptr_type::allocator_type>::value
          || !std::is_rvalue_reference<typename U::node_ptr_type>::value))
      {
        // If not the same allocator or not moveable, need to reallocate
        bool needToRealloc=!std::is_same<Alloc, typename U::node_ptr_type::allocator_type>::value
          || !std::is_rvalue_reference<typename U::node_ptr_type>::value;
        if(!needToRealloc)
        {
          node_ptr_type ret(_allocator);
          ret.p=p.release();
          return ret;
        }
        return node_ptr_type(_allocator, std::forward<typename U::value_type>(*p));
      }
      /*! \brief Factory function for many node_ptr_types, optionally using an array of preexisting
      memory allocations which must be deallocatable by this container's allocator.
      */
      template<class InputIterator> std::vector<node_ptr_type> make_node_ptrs(InputIterator start, InputIterator finish, value_type **to_use=nullptr)
      {
        static_assert(std::is_same<typename std::decay<typename InputIterator::value_type>::type, value_type>::value, "InputIterator type is not my value_type");
        std::vector<node_ptr_type> ret;
        // If the iterator is capable of random access, reserve the vector
        if(std::is_constructible<std::random_access_iterator_tag, typename std::iterator_traits<InputIterator>::iterator_category>::value)
        {
          size_type len=std::distance(start, finish);
          ret.reserve(len);
        }
        for(; start!=finish; ++start, to_use ? ++to_use : to_use)
        {
          if(to_use)
          {
            ret.push_back(node_ptr_type(_allocator));
            _allocator.construct(*to_use, std::forward<typename InputIterator::value_type>(*start));
            ret.back().p=*to_use;
          }
          else
            ret.push_back(node_ptr_type(_allocator, std::forward<typename InputIterator::value_type>(*start)));
        }
        return ret;
      }
    private:
      template<class C> std::pair<iterator, bool> _insert(node_ptr_type &&v, C &&extend)
      {
        std::pair<iterator, bool> ret(end(), false);
        const key_type &k=v->first;
        size_t h=_hasher(k);
        bool done=false;
        do
        {
          auto itb=_get_bucket(h);
          bucket_type &b=*itb;
          size_t emptyidx=(size_t) -1;
#if 1
          // First search for equivalents and empties.
          if(b.count.load(memory_order_relaxed))
          {
            size_t offset=0;
            BOOST_BEGIN_TRANSACT_LOCK_ONLY_IF_NOT(b.lock, 2)
              for(item_type *i=b.items.data(), *e=b.items.data()+b.items.size();;)
              {
                for(; i<e && i->hash!=h; offset++, i++)
                  if(emptyidx==(size_t) -1 && !i->p)
                    emptyidx=offset;
                if(i==e)
                  break;
                if(i->p && _key_equal(k, i->p->first))
                {
                  ret.first._itb=itb;
                  ret.first._offset=offset;
                  done=true;
                  break;
                }
                else offset++, i++;
              }
            BOOST_END_TRANSACT_LOCK(b.lock)
          }
          else emptyidx=0;
#endif

          if(!done)
          {
            // If the lock is other state we need to reload bucket list
            if(!b.lock.lock(2))
              continue;
            std::lock_guard<decltype(b.lock)> g(b.lock, std::adopt_lock); // Will abort all concurrency
            // If we earlier found an empty use that
            if(emptyidx!=(size_t) -1)
            {
              item_type *i=b.items.data()+emptyidx;
              if(emptyidx<b.items.size() && !i->p)
              {
                ret.first._itb=itb;
                ret.first._offset=emptyidx;
                ret.second=true;
                i->p=v.release();
                i->hash=h;
                b.count.fetch_add(1, memory_order_relaxed);
                done=true;
              }
            }
            if(!done)
              done=extend(ret, itb, h, std::move(v));
          }
        } while(!done);
        return ret;
      }
    public:
      //! Inserts a node ptr if it is possible without allocating memory. Useful for low latency. If iterator is valid, didn't insert due to key collision.
      std::pair<iterator, bool> insert_noalloc(node_ptr_type &&v) BOOST_NOEXCEPT
      {
        return _insert(std::move(v), [](std::pair<iterator, bool> &ret, typename buckets_type::iterator &itb, size_t h, node_ptr_type &&v){ return true; });
      }
      std::pair<iterator, bool> insert(node_ptr_type &&v)
      {
        return _insert(std::move(v), [](std::pair<iterator, bool> &ret, typename buckets_type::iterator &itb, size_t h, node_ptr_type &&v)
        {
          bucket_type &b=*itb;
          if(b.items.size()==b.items.capacity())
          {
            size_t newcapacity=b.items.capacity()*2;
            b.items.reserve(newcapacity ? newcapacity : 1);
          }
          ret.first._itb=itb;
          ret.first._offset=b.items.size();
          ret.second=true;
          b.items.push_back(item_type(h, std::move(v)));
          b.count.fetch_add(1, memory_order_relaxed);
          return true;
        });
      }

      //! Note this function consumes args if passed by rvalue ref if an exception throws. Use insert() instead if you don't want this.
      template<class... Args> std::pair<iterator, bool> emplace(Args &&... args)
      {
        node_ptr_type n(make_node_ptr(std::forward<Args>(args)...));
        auto u=undoer([&]{}); // FIXME Need to restore args out of node_ptr_type.
        auto ret=insert(std::move(n));
        u.dismissed=ret.second;
        return ret;
      }
      template<class... Args> iterator emplace_hint(const_iterator position, Args &&... args) { return emplace(std::forward<Args>(args)...); }
      //! Inserts a value if it is possible without allocating memory. Useful for low latency. If iterator is valid, didn't insert due to key collision.
      std::pair<iterator, bool> insert_noalloc(const value_type &v) BOOST_NOEXCEPT { return insert_noalloc(value_type(v)); }
      std::pair<iterator, bool> insert(const value_type &v) { return insert(value_type(v)); }
      //! Move inserts a value if it is possible without allocating memory. Useful for low latency. If iterator is valid, didn't insert due to key collision.
      std::pair<iterator, bool> insert_noalloc(value_type &&v) BOOST_NOEXCEPT
      {
        node_ptr_type n(make_node_ptr(std::move(v)));
        auto u=undoer([&]{
#if 1
          v.second=std::move(n->second);
#else
          v.~value_type();
          new(&v) value_type(std::move(*n));
#endif
        });
        auto ret=insert_noalloc(std::move(n));
        u.dismissed=ret.second;
        return ret;
      }
      std::pair<iterator, bool> insert(value_type &&v)
      {
        node_ptr_type n(make_node_ptr(std::move(v)));
        auto u=undoer([&]{
#if 1
          v.second=std::move(n->second);
#else
          v.~value_type();
          new(&v) value_type(std::move(*n));
#endif
        });
        auto ret=insert(std::move(n));
        u.dismissed=ret.second;
        return ret;
      }
      iterator insert(const_iterator hint, const value_type &v) { return insert(v).first; }
      iterator insert(const_iterator hint, value_type &&v) { return insert(std::move(v)).first; }
      template<class InputIterator> void insert(InputIterator first, InputIterator last)
      {
        for(; first!=last; ++first)
          insert(*first);
      }
      void insert(std::initializer_list<value_type> i) { insert(i.begin(), i.end()); }
      //! Extract a value into its node ptr for later insertion into something else
      node_ptr_type extract(const_iterator it) BOOST_NOEXCEPT
      {
        //assert(it!=end());
        if(it==end()) return node_ptr_type();
        bucket_type &b=*it._itb;
        node_ptr_type former(_allocator);
        {
          // If the lock is other state we need to reload bucket list
          if(!b.lock.lock(2))
            abort();
          std::lock_guard<decltype(b.lock)> g(b.lock, std::adopt_lock); // Will abort all concurrency
          auto &items=b.items;
          item_type *i=items.data()+it._offset;
          if(it._offset<items.size() && i->p)
          {
            former.p=i->p;
            i->p=nullptr;
            i->hash=0;
            if(it._offset==items.size()-1)
            {
              // Shrink table to minimum
              while(!items.empty() && !items.back().p)
                items.pop_back(); // Will abort all concurrency
            }
            b.count.fetch_sub(1, memory_order_relaxed);
          }
        }
        return former;
      }
      //! Extract a value into its node ptr for later insertion into something else
      node_ptr_type extract(const key_type &k) BOOST_NOEXCEPT
      {
        size_t h=_hasher(k);
        bool done=false;
        node_ptr_type former(_allocator);
        do
        {
          auto itb=_get_bucket(h);
          bucket_type &b=*itb;
          size_t offset=0;
          if(b.count.load(memory_order_relaxed))
          {
            // If the lock is other state we need to reload bucket list
            if(!b.lock.lock(2))
              continue;
            std::lock_guard<decltype(b.lock)> g(b.lock, std::adopt_lock); // Will abort all concurrency
            {
              auto &items=b.items;
              for(item_type *i=items.data()+offset, *e=items.data()+items.size();;)
              {
                for(; i<e && i->hash!=h; offset++, i++);
                if(i==e)
                  break;
                if(i->p && _key_equal(k, i->p->first))
                {
                  former.p=i->p;
                  i->p=nullptr;
                  i->hash=0;
                  if(offset==items.size()-1)
                  {
                    // Shrink table to minimum
                    while(!items.empty() && !items.back().p)
                      items.pop_back(); // Will abort all concurrency
                  }
                  b.count.fetch_sub(1, memory_order_relaxed);
                  done=true;
                  break;
                }
                else offset++, i++;
              }
              done=true;
            }
          }
        } while(!done);
        return former;
      }
      //! Extract many values as node ptrs for later insertion into something else
      std::vector<node_ptr_type> extract(const_iterator first, const_iterator last) BOOST_NOEXCEPT
      {
        std::vector<node_ptr_type> ret;
        for(; first!=last; ++first)
          ret.push_back(extract(first));
        return ret;
      }
      iterator erase(const_iterator it) BOOST_NOEXCEPT
      {
        iterator ret(it);
        ++ret;
        node_ptr_type e=extract(it);
        return e ? ret : end();
      }
      size_type erase(const key_type &k) BOOST_NOEXCEPT
      {
        node_ptr_type e=extract(k);
        return e ? 1 : 0;
      }
      iterator erase(const_iterator first, const_iterator last) BOOST_NOEXCEPT
      {
        while(first!=last)
          first=erase(first);
        return last;
      }

      void clear() BOOST_NOEXCEPT
      {
        bool done;
        do
        {
          buckets_type &buckets=*_buckets.load(memory_order_consume);
          done=true;
          for(auto &b : buckets)
          {
            // If the lock is other state we need to reload bucket list
            if(!b.lock.lock(2))
            {
              done=false;
              break;
            }
            std::lock_guard<decltype(b.lock)> g(b.lock, std::adopt_lock); // Will abort all concurrency
            for(auto &i : b.items)
            {
              node_ptr_type former(_allocator);
              former.p=i.p;
              i.p=nullptr;
            }
            b.items.clear();
            b.count.store(0, memory_order_relaxed);
          }
        } while(!done);
      }
      void swap(concurrent_unordered_map &o) BOOST_NOEXCEPT
      {
        std::lock(_rehash_lock, o._rehash_lock);
        std::lock_guard<decltype(_rehash_lock)> g1(_rehash_lock, std::adopt_lock);
        std::lock_guard<decltype(o._rehash_lock)> g2(o._rehash_lock, std::adopt_lock);
        std::swap(_hasher, o._hasher);
        std::swap(_key_equal, o._key_equal);
        std::swap(_max_load_factor, o._max_load_factor);
        std::swap(_min_bucket_capacity, o._min_bucket_capacity);
        _buckets.store(o._buckets.exchange(_buckets.load(memory_order_consume), memory_order_acq_rel), memory_order_release);
        // intentionally leave old_buckets as-is
      }
      //! Merges all the values from the source map into the destination map in a highly efficient way. Rehash safe.
      template<class _Hash, class _Pred, class _Alloc> void merge(concurrent_unordered_map<Key, T, _Hash, _Pred, _Alloc> &o)
      {
        typedef concurrent_unordered_map<Key, T, _Hash, _Pred, _Alloc> other_map_type;
        bool done;
        do
        {
          buckets_type &obuckets=*o._buckets.load(memory_order_consume);
          done=true;
          for(auto &b : obuckets)
          {
            // If the lock is other state we need to reload bucket list
            if(!b.lock.lock(2))
            {
              done=false;
              break;
            }
            std::lock_guard<decltype(b.lock)> g(b.lock, std::adopt_lock); // Will abort all concurrency
            auto &items=b.items;
            size_t failed=0;
            for(auto &i : items)
            {
              typename other_map_type::node_ptr_type former(o._allocator);
              former.p=i.p;
              i.p=nullptr;
              node_ptr_type n(rebind_node_ptr<other_map_type>(std::move(former))); // Will reallocate only if necessary
              if(!insert(std::move(n)).second)
              {
                // Insertion failed, put it back
                // FIXME If allocators are different it could throw here during rebind, thus losing the value
                typename other_map_type::node_ptr_type former2(o.template rebind_node_ptr<concurrent_unordered_map>(std::move(n)));
                i.p=former2.release();
                failed++;
              }
            }
            b.count.store(failed, memory_order_relaxed);
            if(!failed)
              items.clear();
            else
            {
              while(!items.empty() && !items.back().p)
                items.pop_back(); // Will abort all concurrency
            }
          }
        } while(!done);
      }
      //! Merges all the values from the source map into the destination map in a highly efficient way. Not rehash safe.
      template<class U> void merge(U &o)
      {
        for(typename U::iterator it=o.begin(); it!=o.end(); it=o.begin())
        {
          typename U::node_ptr_type e=o.extract(it);
          insert(rebind_node_ptr(e));
        }
      }
      size_type bucket_count() const BOOST_NOEXCEPT { return _buckets.load(memory_order_consume)->size(); }
      size_type max_bucket_count() const BOOST_NOEXCEPT { return _buckets.load(memory_order_consume)->max_size(); }
      size_type bucket_size(size_type n) const BOOST_NOEXCEPT
      {
        buckets_type &buckets=*_buckets.load(memory_order_consume);
        bucket_type &b=buckets[n];
        return b.items.count.load(memory_order_relaxed);
      }
      size_type bucket(const key_type &k) const BOOST_NOEXCEPT
      {
        buckets_type &buckets=*_buckets.load(memory_order_consume);
        return _hasher(k) % buckets.size();
      }
      float load_factor() const BOOST_NOEXCEPT { return (float) size()/bucket_count(); }
      float max_load_factor() const BOOST_NOEXCEPT { return _max_load_factor; }
      void max_load_factor(float m) BOOST_NOEXCEPT { _max_load_factor=m; }
      //! The minimum number of empty spaces in a bucket, thus avoiding an allocation on first insert. You need to rehash after setting this.
      size_t min_bucket_capacity() const BOOST_NOEXCEPT { return _min_bucket_capacity; }
      //! The minimum number of empty spaces in a bucket, thus avoiding an allocation on first insert. You need to rehash after setting this.
      void min_bucket_capacity(size_t n) { _min_bucket_capacity=n; }
    private:
      static void _reset_buckets(buckets_type &buckets) BOOST_NOEXCEPT
      {
        for(auto &b : buckets)
        {
          for(auto &i : b.items)
            i.p=nullptr;
          b.items.clear();
          b.items.shrink_to_fit();
          b.count.store(0, memory_order_relaxed);
        }
      }
      // buckets must be locked on entry!
      void _rehash(size_type n)
      {
        // Create a new buckets
        buckets_type *tempbuckets=new buckets_type(n);
        auto untempbuckets=undoer([&tempbuckets]{ delete tempbuckets; });
        // Lock all new buckets
        for(auto &b : *tempbuckets)
        {
          b.lock.lock();
          if(_min_bucket_capacity)
            b.items.reserve(_min_bucket_capacity);
        }
        // Swap old buckets with new buckets
        tempbuckets=_buckets.exchange(tempbuckets, memory_order_acq_rel);
        // Tell all threads using old buckets to start reloading the bucket list
        for(auto &b : *tempbuckets)
          b.lock.store(2);
        // If it's a simple buckets cycle, simply move over all items as it's noexcept
        if(_buckets.load(memory_order_relaxed)->size()==tempbuckets->size())
        {
          // Simply move the old buckets into new buckets as-is
          for(auto obit=tempbuckets->begin(), bit=_buckets.load(memory_order_consume)->begin(); obit!=tempbuckets->end(); ++obit, ++bit)
          {
            auto &ob=*obit;
            auto &b=*bit;
            if(ob.count.load(memory_order_relaxed))
            {
              b.items=std::move(ob.items);
              b.count.store(ob.count.load(memory_order_consume), memory_order_release);
              ob.count.store(0, memory_order_release);
            }
          }
        }
        else
        {
          try
          {
            // Relocate all old bucket contents into new buckets
            for(const auto &ob : *tempbuckets)
            {
              if(ob.count.load(memory_order_relaxed))
              {
                for(auto &i : ob.items)
                {
                  if(i.p)
                  {
                    auto itb=_get_bucket(i.hash);
                    bucket_type &b=*itb;
                    if(b.items.size()==b.items.capacity())
                    {
                      size_t newcapacity=b.items.capacity()*2;
                      b.items.reserve(newcapacity ? newcapacity : 1);
                    }
                    b.items.push_back(item_type(i.hash, i.p));
                    b.count.fetch_add(1, memory_order_relaxed);
                  }
                }
              }
            }
          }
          catch(...)
          {
            // If we saw an exception during relocation, simply restore
            // the old list which is untouched.
            tempbuckets=_buckets.exchange(tempbuckets, memory_order_acq_rel);
            // Stop it deleting stuff
            _reset_buckets(*tempbuckets);
            // Unlock buckets
            for(auto &b : *_buckets)
              b.lock.unlock();
            throw;
          }
        }
        // Stop old buckets deleting stuff
        _reset_buckets(*tempbuckets);
        // Hold onto old buckets for a while
        typename old_buckets_type::iterator myoldbucket=_oldbucketit++;
        if(_oldbucketit==_oldbuckets.end()) _oldbucketit=_oldbuckets.begin();
        delete *myoldbucket;
        *myoldbucket=tempbuckets;
        tempbuckets=nullptr;
        untempbuckets.dismissed=true;
        // Unlock buckets
        for(auto &b : *_buckets)
          b.lock.unlock();
      }
    public:
      //! Rehashes the map to use n buckets. Comes with a ton of caveats, see detailed description.
      void rehash(size_type n)
      {
        std::lock_guard<decltype(_rehash_lock)> g(_rehash_lock); // Stop other rehashes
        buckets_type &buckets=*_buckets.load(memory_order_consume);
        if(n!=buckets.size())
        {
          // Lock all existing buckets
          for(auto &b : buckets)
            b.lock.lock();
          _rehash(n);
        }
      }
      //! Rehashes the map to use n divided by max_load_factor buckets. Comes with a ton of caveats, see detailed description.
      void reserve(size_type n)
      {
        rehash((size_type)(n/_max_load_factor));
      }
      // trim() // TODO Trims storage to minimum, invalidating all iterators
      //! Reduce the storage of old buckets from rehashing to the amount requested
      void trim_old_rehash_buckets(size_type remaining=0)
      {
        size_type toclear=(_oldbuckets.size()>remaining) ? _oldbuckets.size()-remaining : 0;
        if(toclear)
        {
          std::lock_guard<decltype(_rehash_lock)> g(_rehash_lock); // Stop other rehashes
          typename old_buckets_type::iterator it=_oldbucketit;
          while(toclear--)
          {
            if(++it==_oldbuckets.end()) it=_oldbuckets.begin();
            delete *it;
            *it=nullptr;
          }
        }
      }
      hasher hash_function() const { return _hasher; }
      key_equal key_eq() const { return _key_equal; }
      allocator_type get_allocator() const BOOST_NOEXCEPT { return _allocator; }
      void dump_buckets(std::ostream &s) const
      {
        buckets_type &buckets=*_buckets.load(memory_order_consume);
        for(size_t n=0; n<buckets.size(); n++)
        {
          s << "Bucket " << n << ": size=" << buckets[n].items.size() << " count=" << buckets[n].count << std::endl;
        }
      }
    }; // concurrent_unordered_map

BOOST_SPINLOCK_V1_NAMESPACE_END

#if 0
namespace std
{
  template <class Key, class T, class Hash, class Pred, class Alloc>
  void swap(boost::spinlock::concurrent_unordered_map<Key, T, Hash, Pred, Alloc>& lhs, boost::spinlock::concurrent_unordered_map<Key, T, Hash, Pred, Alloc>& rhs)
  {
    lhs.swap(rhs);
  }
}
#endif

#endif // BOOST_SPINLOCK_HPP

#ifdef BOOST_SPINLOCK_MAP_NAMESPACE_BEGIN
#include "spinlock.bind.hpp"
#endif
