/* spinlock.hpp
Provides yet another spinlock
(C) 2013-2014 Niall Douglas http://www.nedprod.com/
File Created: Sept 2013


Boost Software License - Version 1.0 - August 17th, 2003

Permission is hereby granted, free of charge, to any person or organization
obtaining a copy of the software and accompanying documentation covered by
this license (the "Software") to use, reproduce, display, distribute,
execute, and transmit the Software, and to prepare derivative works of the
Software, and to permit third-parties to whom the Software is furnished to
do so, all subject to the following:

The copyright notices in the Software and this entire statement, including
the above license grant, this restriction and the following disclaimer,
must be included in all copies of the Software, in whole or in part, and
all derivative works of the Software, unless such copies or derivative
works are solely in the form of machine-executable object code generated by
a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS IN THE SOFTWARE.
*/

#ifndef BOOST_SPINLOCK_HPP
#define BOOST_SPINLOCK_HPP

#include <cassert>
#include <vector>
#include <memory>

#if !defined(BOOST_SPINLOCK_USE_BOOST_ATOMIC) && defined(BOOST_NO_CXX11_HDR_ATOMIC)
# define BOOST_SPINLOCK_USE_BOOST_ATOMIC
#endif
#if !defined(BOOST_SPINLOCK_USE_BOOST_THREAD) && defined(BOOST_NO_CXX11_HDR_THREAD)
# define BOOST_SPINLOCK_USE_BOOST_THREAD
#endif

#ifdef BOOST_SPINLOCK_USE_BOOST_ATOMIC
# include <boost/atomic.hpp>
#else
# include <atomic>
#endif
#ifdef BOOST_SPINLOCK_USE_BOOST_THREAD
# include <boost/thread.hpp>
#else
# include <thread>
# include <chrono>
#endif

// For lock_guard
#include <mutex>
// For dump
#include <ostream>


// Turn this on if you have a compiler which understands __transaction_relaxed
//#define BOOST_HAVE_TRANSACTIONAL_MEMORY_COMPILER

#ifndef BOOST_SMT_PAUSE
# if defined(_MSC_VER) && _MSC_VER >= 1310 && ( defined(_M_IX86) || defined(_M_X64) )
extern "C" void _mm_pause();
#  pragma intrinsic( _mm_pause )
#  define BOOST_SMT_PAUSE _mm_pause();
# elif defined(__GNUC__) && ( defined(__i386__) || defined(__x86_64__) )
#  define BOOST_SMT_PAUSE __asm__ __volatile__( "rep; nop" : : : "memory" );
# endif
#endif

#ifndef BOOST_NOEXCEPT
# if !defined(_MSC_VER) || _MSC_VER >= 1900
#  define BOOST_NOEXCEPT noexcept
# endif
#endif
#ifndef BOOST_NOEXCEPT
# define BOOST_NOEXCEPT
#endif

#ifndef BOOST_NOEXCEPT_OR_NOTHROW
# if !defined(_MSC_VER) || _MSC_VER >= 1900
#  define BOOST_NOEXCEPT_OR_NOTHROW noexcept
# endif
#endif
#ifndef BOOST_NOEXCEPT_OR_NOTHROW
# define BOOST_NOEXCEPT_OR_NOTHROW throw()
#endif

#ifndef BOOST_CONSTEXPR
# if !defined(_MSC_VER) || _MSC_VER >= 1900
#  define BOOST_CONSTEXPR constexpr
# endif
#endif
#ifndef BOOST_CONSTEXPR
# define BOOST_CONSTEXPR
#endif

#ifndef BOOST_CONSTEXPR_OR_CONST
# if !defined(_MSC_VER) || _MSC_VER >= 1900
#  define BOOST_CONSTEXPR_OR_CONST constexpr
# endif
#endif
#ifndef BOOST_CONSTEXPR_OR_CONST
# define BOOST_CONSTEXPR_OR_CONST const
#endif

namespace boost
{
  namespace spinlock
  {
    // Map in an atomic implementation
    template <class T>
    class atomic
#ifdef BOOST_SPINLOCK_USE_BOOST_ATOMIC
      : public boost::atomic<T>
    {
      typedef boost::atomic<T> Base;
#else
      : public std::atomic<T>
    {
      typedef std::atomic<T> Base;
#endif

    public:
      atomic() : Base() {}
      BOOST_CONSTEXPR atomic(T v) BOOST_NOEXCEPT : Base(std::forward<T>(v)) {}

#ifdef BOOST_NO_CXX11_DELETED_FUNCTIONS
      //private:
      atomic(const Base &) /* =delete */;
#else
      atomic(const Base &) = delete;
#endif
    };//end boost::afio::atomic
#ifdef BOOST_SPINLOCK_USE_BOOST_ATOMIC
    using boost::memory_order;
    using boost::memory_order_relaxed;
    using boost::memory_order_consume;
    using boost::memory_order_acquire;
    using boost::memory_order_release;
    using boost::memory_order_acq_rel;
    using boost::memory_order_seq_cst;
#else
    using std::memory_order;
    using std::memory_order_relaxed;
    using std::memory_order_consume;
    using std::memory_order_acquire;
    using std::memory_order_release;
    using std::memory_order_acq_rel;
    using std::memory_order_seq_cst;
#endif
    // Map in a this_thread implementation
#ifdef BOOST_SPINLOCK_USE_BOOST_THREAD
    namespace this_thread=boost::this_thread;
    namespace chrono { using boost::chrono::milliseconds; }
#else
    namespace this_thread=std::this_thread;
    namespace chrono { using std::chrono::milliseconds; }
#endif


    /*! \struct lockable_ptr
     * \brief Lets you use a pointer to memory as a spinlock :)
     */
    template<typename T> struct lockable_ptr : atomic<T *>
    {
      lockable_ptr(T *v=nullptr) : atomic<T *>(v) { }
      //! Returns the memory pointer part of the atomic
      T *get() BOOST_NOEXCEPT_OR_NOTHROW
      {
        union
        {
          T *v;
          size_t n;
        } value;
        value.v=atomic<T *>::load();
        value.n&=~(size_t)1;
        return value.v;
      }
      //! Returns the memory pointer part of the atomic
      const T *get() const BOOST_NOEXCEPT_OR_NOTHROW
      {
        union
        {
          T *v;
          size_t n;
        } value;
        value.v=atomic<T *>::load();
        value.n&=~(size_t)1;
        return value.v;
      }
      T &operator*() BOOST_NOEXCEPT_OR_NOTHROW { return *get(); }
      const T &operator*() const BOOST_NOEXCEPT_OR_NOTHROW { return *get(); }
      T *operator->() BOOST_NOEXCEPT_OR_NOTHROW { return get(); }
      const T *operator->() const BOOST_NOEXCEPT_OR_NOTHROW { return get(); }
    };
    template<typename T> struct spinlockbase
    {
    protected:
      atomic<T> v;
    public:
      typedef T value_type;
      spinlockbase() BOOST_NOEXCEPT_OR_NOTHROW : v(0)
      { }
      spinlockbase(const spinlockbase &) = delete;
      //! Atomically move constructs
      spinlockbase(spinlockbase &&o) BOOST_NOEXCEPT_OR_NOTHROW : v(0)
      {
        //v.store(o.v.exchange(0, memory_order_acq_rel));
      }
      //! Returns the raw atomic
      T load(memory_order o=memory_order_seq_cst) const BOOST_NOEXCEPT_OR_NOTHROW { return v.load(o); }
      //! Sets the raw atomic
      void store(T a, memory_order o=memory_order_seq_cst) BOOST_NOEXCEPT_OR_NOTHROW { v.store(a, o); }
      //! If atomic is zero, sets to 1 and returns true, else false.
      bool try_lock() BOOST_NOEXCEPT_OR_NOTHROW
      {
        if(v.load(memory_order_acquire)) // Avoid unnecessary cache line invalidation traffic
          return false;
        T expected=0;
        return v.compare_exchange_weak(expected, 1, memory_order_acquire, memory_order_consume);
      }
      //! If atomic equals expected, sets to 1 and returns true, else false with expected updated to actual value.
      bool try_lock(T &expected) BOOST_NOEXCEPT_OR_NOTHROW
      {
        T t;
        if((t=v.load(memory_order_acquire))) // Avoid unnecessary cache line invalidation traffic
        {
          expected=t;
          return false;
        }
        return v.compare_exchange_weak(expected, 1, memory_order_acquire, memory_order_consume);
      }
      //! Sets the atomic to zero
      void unlock() BOOST_NOEXCEPT_OR_NOTHROW
      {
        v.store(0, memory_order_release);
      }
      bool int_yield(size_t) BOOST_NOEXCEPT_OR_NOTHROW { return false; }
    };
    template<typename T> struct spinlockbase<lockable_ptr<T>>
    {
    private:
      lockable_ptr<T> v;
    public:
      typedef T *value_type;
      spinlockbase() BOOST_NOEXCEPT_OR_NOTHROW { }
      spinlockbase(const spinlockbase &) = delete;
      //! Atomically move constructs
      spinlockbase(spinlockbase &&o) BOOST_NOEXCEPT_OR_NOTHROW
      {
        v.store(o.v.exchange(nullptr, memory_order_acq_rel));
      }
      //! Returns the memory pointer part of the atomic
      T *get() BOOST_NOEXCEPT_OR_NOTHROW { return v.get(); }
      T *operator->() BOOST_NOEXCEPT_OR_NOTHROW { return get(); }
      //! Returns the raw atomic
      T *load(memory_order o=memory_order_seq_cst) BOOST_NOEXCEPT_OR_NOTHROW { return v.load(o); }
      //! Sets the memory pointer part of the atomic preserving lockedness
      void set(T *a) BOOST_NOEXCEPT_OR_NOTHROW
      {
        union
        {
          T *v;
          size_t n;
        } value;
        T *expected;
        do
        {
          value.v=v.load();
          expected=value.v;
          bool locked=value.n&1;
          value.v=a;
          if(locked) value.n|=1;
        } while(!v.compare_exchange_weak(expected, value.v));
      }
      //! Sets the raw atomic
      void store(T *a, memory_order o=memory_order_seq_cst) BOOST_NOEXCEPT_OR_NOTHROW { v.store(a, o); }
      bool try_lock() BOOST_NOEXCEPT_OR_NOTHROW
      {
        union
        {
          T *v;
          size_t n;
        } value;
        value.v=v.load();
        if(value.n&1) // Avoid unnecessary cache line invalidation traffic
          return false;
        T *expected=value.v;
        value.n|=1;
        return v.compare_exchange_weak(expected, value.v);
      }
      void unlock() BOOST_NOEXCEPT_OR_NOTHROW
      {
        union
        {
          T *v;
          size_t n;
        } value;
        value.v=v.load();
        assert(value.n&1);
        value.n&=~(size_t)1;
        v.store(value.v);
      }
      bool int_yield(size_t) BOOST_NOEXCEPT_OR_NOTHROW { return false; }
    };
    //! \brief How many spins to loop, optionally calling the SMT pause instruction on Intel
    template<size_t spins, bool use_pause=true> struct spins_to_loop
    {
      template<class parenttype> struct policy : parenttype
      {
        static BOOST_CONSTEXPR_OR_CONST size_t spins_to_loop=spins;
        policy() {}
        policy(const policy &) = delete;
        policy(policy &&o) BOOST_NOEXCEPT : parenttype(std::move(o)) { }
        bool int_yield(size_t n) BOOST_NOEXCEPT_OR_NOTHROW
        {
          if(parenttype::int_yield(n)) return true;
          if(n>=spins) return false;
          if(use_pause)
          {
#ifdef BOOST_SMT_PAUSE
            BOOST_SMT_PAUSE;
#endif
          }
          return true;
        }
      };
    };
    //! \brief How many spins to yield the current thread's timeslice
    template<size_t spins> struct spins_to_yield
    {
      template<class parenttype> struct policy : parenttype
      {
        static BOOST_CONSTEXPR_OR_CONST size_t spins_to_yield=spins;
        policy() {}
        policy(const policy &) = delete;
        policy(policy &&o) BOOST_NOEXCEPT : parenttype(std::move(o)) { }
        bool int_yield(size_t n) BOOST_NOEXCEPT_OR_NOTHROW
        {
          if(parenttype::int_yield(n)) return true;
          if(n>=spins) return false;
          this_thread::yield();
          return true;
        }
      };
    };
    //! \brief How many spins to sleep the current thread
    struct spins_to_sleep
    {
      template<class parenttype> struct policy : parenttype
      {
        policy() {}
        policy(const policy &) = delete;
        policy(policy &&o) BOOST_NOEXCEPT : parenttype(std::move(o)) { }
        bool int_yield(size_t n) BOOST_NOEXCEPT_OR_NOTHROW
        {
          if(parenttype::int_yield(n)) return true;
          this_thread::sleep_for(chrono::milliseconds(1));
          return true;
        }
      };
    };
    //! \brief A spin policy which does nothing
    struct null_spin_policy
    {
      template<class parenttype> struct policy : parenttype
      {
      };
    };
    /*! \class spinlock
    
    Meets the requirements of BasicLockable and Lockable. Also provides a get() and set() for the
    type used for the spin lock.

    So what's wrong with boost/smart_ptr/detail/spinlock.hpp then, and why
    reinvent the wheel?

    1. Non-configurable spin. AFIO needs a bigger spin than smart_ptr provides.

    2. AFIO is C++ 11, and therefore can implement this in pure C++ 11 atomics.

    3. I don't much care for doing writes during the spin. It generates an
    unnecessary amount of cache line invalidation traffic. Better to spin-read
    and only write when the read suggests you might have a chance.
    
    4. This spin lock can use a pointer to memory as the spin lock. See locked_ptr<T>.
    */
    template<typename T, template<class> class spinpolicy2=spins_to_loop<125>::policy, template<class> class spinpolicy3=spins_to_yield<250>::policy, template<class> class spinpolicy4=spins_to_sleep::policy> class spinlock : public spinpolicy4<spinpolicy3<spinpolicy2<spinlockbase<T>>>>
    {
      typedef spinpolicy4<spinpolicy3<spinpolicy2<spinlockbase<T>>>> parenttype;
    public:
      spinlock() { }
      spinlock(const spinlock &) = delete;
      spinlock(spinlock &&o) BOOST_NOEXCEPT : parenttype(std::move(o)) { }
      void lock() BOOST_NOEXCEPT_OR_NOTHROW
      {
        for(size_t n=0;; n++)
        {
          if(parenttype::try_lock())
            return;
          parenttype::int_yield(n);
        }
      }
      //! Locks if the atomic is not the supplied value, else returning false
      bool lock(T only_if_not_this) BOOST_NOEXCEPT_OR_NOTHROW
      {
        for(size_t n=0;; n++)
        {
          T expected=0;
          if(parenttype::try_lock(expected))
            return true;
          if(expected==only_if_not_this)
            return false;
          parenttype::int_yield(n);
        }
      }
    };

    //! \brief Determines if a lockable is locked. Type specialise this for performance if your lockable allows examination.
    template<class T> inline bool is_lockable_locked(T &lockable) BOOST_NOEXCEPT_OR_NOTHROW
    {
      if(lockable.try_lock())
      {
        lockable.unlock();
        return true;
      }
      return false;
    }
    // For when used with a spinlock
    template<class T, template<class> class spinpolicy2, template<class> class spinpolicy3, template<class> class spinpolicy4> inline T is_lockable_locked(spinlock<T, spinpolicy2, spinpolicy3, spinpolicy4> &lockable) BOOST_NOEXCEPT_OR_NOTHROW
    {
#ifdef BOOST_HAVE_TRANSACTIONAL_MEMORY_COMPILER
      // Annoyingly the atomic ops are marked as unsafe for atomic transactions, so ...
      return *((volatile T *) &lockable);
#else
      return lockable.load(memory_order_acquire);
#endif
    }
    // For when used with a locked_ptr
    template<class T, template<class> class spinpolicy2, template<class> class spinpolicy3, template<class> class spinpolicy4> inline bool is_lockable_locked(spinlock<lockable_ptr<T>, spinpolicy2, spinpolicy3, spinpolicy4> &lockable) BOOST_NOEXCEPT_OR_NOTHROW
    {
      return ((size_t) lockable.load(memory_order_acquire))&1;
    }

#ifndef BOOST_BEGIN_TRANSACT_LOCK
#ifdef BOOST_HAVE_TRANSACTIONAL_MEMORY_COMPILER
#undef BOOST_USING_INTEL_TSX
#define BOOST_BEGIN_TRANSACT_LOCK(lockable) __transaction_relaxed { (void) boost::spinlock::is_lockable_locked(lockable); {
#define BOOST_BEGIN_TRANSACT_LOCK_ONLY_IF_NOT(lockable, only_if_not_this) __transaction_relaxed { if((only_if_not_this)!=boost::spinlock::is_lockable_locked(lockable)) {
#define BOOST_END_TRANSACT_LOCK(lockable) } }
#define BOOST_BEGIN_NESTED_TRANSACT_LOCK(N) __transaction_relaxed
#define BOOST_END_NESTED_TRANSACT_LOCK(N)
#endif // BOOST_BEGIN_TRANSACT_LOCK
#endif

#ifndef BOOST_BEGIN_TRANSACT_LOCK
#define BOOST_BEGIN_TRANSACT_LOCK(lockable) { std::lock_guard<decltype(lockable)> __tsx_transaction(lockable);
#define BOOST_BEGIN_TRANSACT_LOCK_ONLY_IF_NOT(lockable, only_if_not_this) if(lockable.lock(only_if_not_this)) { std::lock_guard<decltype(lockable)> __tsx_transaction(lockable, std::adopt_lock);
#define BOOST_END_TRANSACT_LOCK(lockable) }
#define BOOST_BEGIN_NESTED_TRANSACT_LOCK(N)
#define BOOST_END_NESTED_TRANSACT_LOCK(N)
#endif // BOOST_BEGIN_TRANSACT_LOCK

    /* \class concurrent_unordered_map
    \brief Provides an unordered_map which is thread safe and wait free to use and whose find, insert/emplace and erase functions are usually wait free.

    Some notes on this implementation:
    * Provides the N3645 (http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3645.pdf) extensions node_ptr_type, extract() and merge() with
      overload of insert(). Also added is a make_node_ptr() factory function with a rebind/realloc overload, and a make_node_ptrs() batch factory function for those
      with a malloc capable of burst/batch allocation (e.g. Linux, OS X). Finally, merge() is now templatised and rebinds/reallocs node ptrs if necessary.

    * To help you make sane concurrent use of the map, insert/emplace and erase never invalidate iterators nor references. This implies that rehashing is
      manual. If you perform a manual rehash which will invalidate all iterators, you must manually call it at a time when the following conditions are true:
      
      1. All iterators are not in use. That means an erase(find(value)) cannot be happening. Use erase(value) instead.

    * Given the costs of rehashing, the design sacrifices low load factor performance for high load factor performance. Load factors of 16 or so are not
      significantly slower than load factors of less than 1 if given a well distributed hash function.

    * All operations will operate safely in concurrency with all other operations including rehash(). You may get unstable outcomes of course,
      especially if you are inserting and deleting the same key from multiple threads, and no locking is provided per key-value pair, so if you delete it
      from one thread while other threads have references to it you enter undefined behaviour, and probable memory corruption. Strongly consider the use
      of shared_ptr<> as the mapped type if this is a problem for you.

    * All operations may operate concurrently with all other operations except rehash(). If they hit the same bucket they are serialised for obvious reasons. Also
      for obvious reasons a rehash() must halt all concurrency, it is the only operation which does this.

    * To very substantially improve concurrency, the following deviations from std::unordered_map<> behaviour have been made:
      * empty() has average complexity O(bucket count/item count/2), worst case O(bucket count) when the map is empty.
      * size() always has complexity O(bucket count). If you do rehash(size()) to make load factor to 1.0, remember this can become very slow for large
        numbers of items. The map is deliberately more tolerant than most to collisions, it can cope with load factors of 8.0 or so without much slowdown.
      * emplace() consumes its rvalue referenced items even when an exception is thrown i.e. the "has no effect" rule is violated. The map itself is untouched
        however. Chances are real world code will never notice this, but if you do, insert() correctly does not consume arguments if exceptions are thrown.

    * clear() and merge() both are safe concurrent with all other operations, however inserting items concurrently to a clear() or merge()
      has a possibility of losing and failing to merge some of those newly inserted items. erase() is safe however.

    * Not everything is fully implemented and is marked as FIXME hopefully for some future GSoC student. In particular:
      * C++ 03 support
      * const iterators are typedefed to be normal iterators as const iterators haven't been implemented yet
      * All const member functions are const_casted to their non-const forms
      * Local iterators
      * Copy and move construction plus copy and move assignment
      * noexcept is always on in many places it should be conditional. We are blocked on MSVC for this.
    */
    template<class Key, class T, class Hash=std::hash<Key>, class Pred=std::equal_to<Key>, class Alloc=std::allocator<std::pair<const Key, T>>> class concurrent_unordered_map
    {
    public:
      typedef Key key_type;
      typedef T mapped_type;
      typedef std::pair<const key_type, mapped_type> value_type;
      typedef Hash hasher;
      typedef Pred key_equal;
      typedef Alloc allocator_type;

      typedef value_type& reference;
      typedef const value_type& const_reference;
      typedef value_type* pointer;
      typedef const value_type *const_pointer;
      typedef std::size_t size_type;
      typedef std::ptrdiff_t difference_type;
    private:
      spinlock<bool> _rehash_lock;
      hasher _hasher;
      key_equal _key_equal;
      allocator_type _allocator;
      float _max_load_factor;
    public:
      class node_ptr_type
      {
        friend class concurrent_unordered_map;
        typedef concurrent_unordered_map container;
      public:
        typedef typename container::value_type value_type;
        typedef typename container::allocator_type allocator_type;
        typedef typename container::reference reference;
        typedef typename container::pointer pointer;
      private:
        typedef value_type container_node_type;
        allocator_type allocator;
        value_type *p;
        node_ptr_type(allocator_type &_allocator) : allocator(_allocator), p(nullptr) { }
        template<class... Args> node_ptr_type(allocator_type &_allocator, Args... args) : allocator(_allocator), p(nullptr)
        {
          p=allocator.allocate(1);
          try
          {
            allocator.construct(p, std::forward<Args>(args)...);
          }
          catch(...)
          {
            allocator.deallocate(p, 1);
            throw;
          }
        }
      public:
        BOOST_CONSTEXPR node_ptr_type() BOOST_NOEXCEPT : p(nullptr) {}
        BOOST_CONSTEXPR node_ptr_type(std::nullptr_t) BOOST_NOEXCEPT : p(nullptr) {}
        node_ptr_type(node_ptr_type &&o) BOOST_NOEXCEPT : allocator(std::move(o.allocator)), p(o.p) { o.p=nullptr; }
        node_ptr_type(const node_ptr_type &)=delete;
        node_ptr_type &operator=(node_ptr_type &&o) BOOST_NOEXCEPT // FIXME noexcept should depend on value_type noexceptness
        {
          this->~node_ptr_type();
          new(this) value_type(std::move(o));
          return *this;
        }
        node_ptr_type &operator=(std::nullptr_t) BOOST_NOEXCEPT
        {
          reset();
          return *this;
        }
        node_ptr_type &operator=(const node_ptr_type &o)=delete;
        ~node_ptr_type() BOOST_NOEXCEPT
        {
          reset();
        }
        pointer get() const BOOST_NOEXCEPT { return p; }
        reference operator*() BOOST_NOEXCEPT { return *p; }
        pointer operator->() BOOST_NOEXCEPT { return p; }
        allocator_type get_allocator() const BOOST_NOEXCEPT { return allocator; }
        explicit operator bool() const BOOST_NOEXCEPT { return p!=nullptr; }
        pointer release() BOOST_NOEXCEPT
        {
          value_type *ret=p;
          p=nullptr;
          return ret;
        }
        void reset() BOOST_NOEXCEPT
        {
          if(p)
          {
            allocator.destroy(p);
            allocator.deallocate(p, 1);
            p=nullptr;
          }
        }
        void reset(std::nullptr_t) BOOST_NOEXCEPT { reset(); }
        void swap(node_ptr_type &o) BOOST_NOEXCEPT
        {
          node_ptr_type temp(std::move(*this));
          *this=std::move(o);
          o=std::move(temp);
        }
      };
      template<class U> class noalloc : public U
      {
      public:
        explicit noalloc(U &&o) : U(std::move(o)) { }
      };
    private:
      struct item_type
      {
        value_type *p;
        size_t hash;
        item_type() BOOST_NOEXCEPT : p(nullptr), hash(0) { }
        item_type(size_t _hash, value_type *_p) BOOST_NOEXCEPT : p(_p), hash(_hash) { }
        item_type(size_t _hash, node_ptr_type &&_p) BOOST_NOEXCEPT : p(_p.release()), hash(_hash) { }
        item_type(item_type &&o) BOOST_NOEXCEPT : p(std::move(o.p)), hash(o.hash) { o.p=nullptr; o.hash=0; }
        item_type(const item_type &o) = delete;
        ~item_type() BOOST_NOEXCEPT
        {
          assert(!p);
        }
      };
      struct bucket_type_impl
      {
        spinlock<unsigned char> lock;  // = 2 if you need to reload the bucket list
        atomic<unsigned> count; // count is used items in there
        std::vector<item_type> items;
        bucket_type_impl() : count(0), items(0) { }
        bucket_type_impl(bucket_type_impl &&) BOOST_NOEXCEPT : count(0) { }
      };
#if 1 // improves concurrent write performance
      struct bucket_type : bucket_type_impl
      {
        char pad[64-sizeof(bucket_type_impl)];
        bucket_type()
        {
          static_assert(sizeof(bucket_type)==64, "bucket_type is not 64 bytes long!");
        }
      };
#else
      typedef bucket_type_impl bucket_type;
#endif
      std::vector<bucket_type> _buckets;
      typename std::vector<bucket_type>::iterator _get_bucket(size_t k) BOOST_NOEXCEPT
      {
        //k ^= k + 0x9e3779b9 + (k<<6) + (k>>2); // really need to avoid sequential keys tapping the same cache line
        //k ^= k + 0x9e3779b9; // really need to avoid sequential keys tapping the same cache line
        size_type i=k % _buckets.size();
        return _buckets.begin()+i;
      }
      static float _calc_max_load_factor() BOOST_NOEXCEPT
      {
        return 1.0f;
#if 1
        // We are intentionally very tolerant to load factor, so set to
        // however many item_type's fit into 128 bytes
        float ret=128/sizeof(item_type);
        if(ret<1) ret=0;
        return ret;
#endif
      }
    public:
      class iterator : public std::iterator<std::forward_iterator_tag, value_type, difference_type, pointer, reference>
      {
        concurrent_unordered_map *_parent;
        bucket_type *_bucket_data; // used for sanity check that he hasn't rehashed
        typename std::vector<bucket_type>::iterator _itb;
        size_t _offset, _pending_incr; // used to avoid erase() doing a costly increment unless necessary
        friend class concurrent_unordered_map;
        iterator(const concurrent_unordered_map *parent) : _parent(const_cast<concurrent_unordered_map *>(parent)), _bucket_data(_parent->_buckets.data()), _itb(_parent->_buckets.begin()), _offset((size_t) -1), _pending_incr(1) { }
        iterator(const concurrent_unordered_map *parent, std::nullptr_t) : _parent(const_cast<concurrent_unordered_map *>(parent)), _bucket_data(_parent->_buckets.data()), _itb(_parent->_buckets.end()), _offset((size_t) -1), _pending_incr(0) { }
        void _catch_up()
        {
          while(_pending_incr && _itb!=_parent->_buckets.end())
          {
            assert(_bucket_data==_parent->_buckets.data());
            if(_bucket_data!=_parent->_buckets.data())
              abort(); // stale iterator
            bucket_type &b=*_itb;
            BOOST_BEGIN_TRANSACT_LOCK_ONLY_IF_NOT(b.lock, 2)
            {
              auto &items=b.items;
              _offset++;
              for(item_type *i=items.data()+_offset, *e=items.data()+items.size(); i<e; _offset++, i++)
                if(i->p)
                  if(!(--_pending_incr)) break;
            }
            BOOST_END_TRANSACT_LOCK(b.lock)
            if(_pending_incr)
            {
              do
              {
                ++_itb;
                _offset=(size_t) -1;
              } while(_itb!=_parent->_buckets.end() && !_itb->count.load(memory_order_acquire));
            }
          }
        }
        void _catch_up() const { const_cast<iterator *>(this)->_catch_up(); }
      public:
        iterator() : _parent(nullptr), _bucket_data(nullptr), _offset((size_t) -1), _pending_incr(0) { }
        bool operator!=(const iterator &o) const BOOST_NOEXCEPT { _catch_up(); return _itb!=o._itb || _offset!=o._offset; }
        bool operator==(const iterator &o) const BOOST_NOEXCEPT { _catch_up(); return _itb==o._itb && _offset==o._offset; }
        iterator &operator++()
        {
          if(_itb==_parent->_buckets.end())
            return *this;
          ++_pending_incr;
          return *this;
        }
        iterator operator++(int) { iterator t(*this); operator++(); return t; }
        value_type &operator*() { _catch_up(); assert(_itb!=_parent->_buckets.end()); if(_itb==_parent->_buckets.end()) abort(); return *_itb->items[_offset].p; }
        value_type *operator->() { _catch_up(); assert(_itb!=_parent->_buckets.end()); if(_itb==_parent->_buckets.end()) abort(); return _itb->items[_offset].p; }
      };
      typedef iterator const_iterator; // FIXME
      // local_iterator
      // const_local_iterator
      concurrent_unordered_map() : _max_load_factor(_calc_max_load_factor()), _buckets(13) { }
      explicit concurrent_unordered_map(size_type n, const hasher &h=hasher(), const key_equal &ke=key_equal(), const allocator_type &al=allocator_type()) : _hasher(h), _key_equal(ke), _allocator(al), _max_load_factor(_calc_max_load_factor()), _buckets(n>0 ? n : 13) { }
      explicit concurrent_unordered_map(const allocator_type &al) : _allocator(al), _max_load_factor(_calc_max_load_factor()), _buckets(13) { }
      concurrent_unordered_map(size_type n, const allocator_type &al) : _allocator(al), _max_load_factor(_calc_max_load_factor()), _buckets(n>0 ? n : 13) { }
      concurrent_unordered_map(size_type n, const hasher &h, const allocator_type &al) : _hasher(h), _allocator(al), _max_load_factor(_calc_max_load_factor()), _buckets(n>0 ? n : 13) { }

      template<class InputIterator> concurrent_unordered_map(InputIterator first, InputIterator last, size_type n=0, const hasher &h=hasher(), const key_equal &ke=key_equal(), const allocator_type &al=allocator_type()) : _hasher(h), _key_equal(ke), _allocator(al), _max_load_factor(_calc_max_load_factor()), _buckets(n>0 ? n : 13) { insert(first, last); }
      template<class InputIterator> concurrent_unordered_map(InputIterator first, InputIterator last, size_type n, const allocator_type &al) : _allocator(al), _max_load_factor(_calc_max_load_factor()), _buckets(n>0 ? n : 13) { insert(first, last); }
      template<class InputIterator> concurrent_unordered_map(InputIterator first, InputIterator last, size_type n, const hasher &h, const allocator_type &al) : _hasher(h), _allocator(al), _max_load_factor(_calc_max_load_factor()), _buckets(n>0 ? n : 13) { insert(first, last); }
      
      concurrent_unordered_map(std::initializer_list<value_type> il, size_type n=0, const hasher &h=hasher(), const key_equal &ke=key_equal(), const allocator_type &al=allocator_type()) : _hasher(h), _key_equal(ke), _allocator(al), _max_load_factor(_calc_max_load_factor()), _buckets(n>0 ? n : 13) { insert(std::move(il)); }
      concurrent_unordered_map(std::initializer_list<value_type> il, size_type n, const allocator_type &al) : _allocator(al), _max_load_factor(_calc_max_load_factor()), _buckets(n>0 ? n : 13) { insert(std::move(il)); }
      concurrent_unordered_map(std::initializer_list<value_type> il, size_type n, const hasher &h, const allocator_type &al) : _hasher(h), _allocator(al), _max_load_factor(_calc_max_load_factor()), _buckets(n>0 ? n : 13) { insert(std::move(il)); }
      ~concurrent_unordered_map()
      {
        // Raise the rehash lock and leave it raised
        _rehash_lock.lock();
        // Lock all existing buckets
        for(auto &b : _buckets)
          b.lock.lock();
        for(auto &b : _buckets)
        {
          for(auto &i : b.items)
          {
            node_ptr_type former(_allocator);
            former.p=i.p;
            i.p=nullptr;
          }
          b.items.clear();
          b.count.store(0, memory_order_release);
        }
        _buckets.clear();
      }
    private:
      // FIXME Awaiting implementation
      concurrent_unordered_map(const concurrent_unordered_map &);
      concurrent_unordered_map(concurrent_unordered_map &&);
      concurrent_unordered_map &operator=(const concurrent_unordered_map &);
      concurrent_unordered_map &operator=(concurrent_unordered_map &&);
    public:
      //! O(bucket count/item count/2)
      bool empty() const BOOST_NOEXCEPT
      {
        bool done;
        do
        {
          done=true;
          for(auto &b : _buckets)
          {
            // If the lock is other state we need to reload bucket list
            if(b.lock.load(memory_order_acquire)==2)
            {
              done=false;
              break;
            }
            if(b.count.load(memory_order_acquire))
              return false;
          }
        } while(!done);
        return true;
      }
      //! O(bucket count)
      size_type size() const BOOST_NOEXCEPT
      {
        size_type ret;
        bool done;
        do
        {
          done=true;
          ret=0;
          for(auto &b : _buckets)
          {
            // If the lock is other state we need to reload bucket list
            if(b.lock.load(memory_order_acquire)==2)
            {
              done=false;
              break;
            }
            ret+=b.count.load(memory_order_acquire);
          }
        } while(!done);
        return ret;
      }
      size_type max_size() const BOOST_NOEXCEPT { return (size_type) -1; }
      iterator begin() BOOST_NOEXCEPT { return iterator(this); }
      const_iterator begin() const BOOST_NOEXCEPT { return const_iterator(this); }
      const_iterator cbegin() const BOOST_NOEXCEPT { return const_iterator(this); }
      iterator end() BOOST_NOEXCEPT { return iterator(this, nullptr); }
      const_iterator end() const BOOST_NOEXCEPT { return const_iterator(this, nullptr); }
      const_iterator cend() const BOOST_NOEXCEPT { return const_iterator(this, nullptr); }
      iterator find(const key_type &k)
      {
        iterator ret=end();
        size_t h=_hasher(k);
        bool done=false;
        do
        {
          auto itb=_get_bucket(h);
          bucket_type &b=*itb;
          size_t offset=0;
          if(!b.count.load(memory_order_acquire))
            done=true;
          else
          {
            // Should run completely concurrently with other finds
            BOOST_BEGIN_TRANSACT_LOCK_ONLY_IF_NOT(b.lock, 2)
            {
              auto &items=b.items;
              for(item_type *i=items.data()+offset, *e=items.data()+items.size();;)
              {
                for(; i<e && i->hash!=h; offset++, i++);
                if(i==e)
                  break;
                if(i->p && _key_equal(k, i->p->first))
                {
                  ret._itb=itb;
                  ret._offset=offset;
                  done=true;
                  break;
                }
                else offset++, i++;
              }
              done=true;
            }
            BOOST_END_TRANSACT_LOCK(b.lock)
          }
        } while(!done);
        return ret;
      }
      const_iterator find(const key_type &k) const { return const_cast<concurrent_unordered_map *>(this)->find(k); } // FIXME
      mapped_type &at(const key_type &k)
      {
        iterator it=find(k);
        if(it==end()) throw std::out_of_range("Key not found");
        return it->second;
      }
      const mapped_type &at(const key_type &k) const { return const_cast<concurrent_unordered_map *>(this)->at(k); } // FIXME
      mapped_type &operator[](const key_type &k)
      {
        do
        {
          iterator it=find(k);
          if(it==end())
          {
            value_type v(k, mapped_type());
            auto ret=insert(std::move(v));
            if(!ret.second)
              continue;
            else
              it=ret.first;
          }
          return it->second;
        } while(false);
        abort();
      }
      mapped_type &operator[](key_type &&k)
      {
        node_ptr_type e=make_node_ptr(value_type(std::move(k), mapped_type()));
        try
        {
          do
          {
            iterator it=find(e->first);
            if(it==end())
            {
              auto ret=insert(std::move(e));
              if(!ret.second)
                continue;
              else
                it=ret.first;
            }
            return it->second;
          } while(false);
        }
        catch(...)
        {
          k=std::move(e->first);
          throw;
        }
        abort();
      }
      size_type count(const key_type &k) const { return end()!=find(k) ? 1 : 0; }
      std::pair<iterator, iterator> equal_range(const key_type &k)
      {
        iterator it=find(k);
        return std::make_pair(it, it);
      }
      std::pair<const_iterator, const_iterator> equal_range(const key_type &k) const
      {
        const_iterator it=find(k);
        return std::make_pair(it, it);
      }

      //! \brief Factory function for a node_ptr_type.
      template<class... Args> node_ptr_type make_node_ptr(Args&&... args)
      {
        return node_ptr_type(_allocator, std::forward<Args>(args)...);
      }
      /*! \brief Lets you rebind a node_ptr_type from another map into this map. If allocators are
      dissimilar or the supplied type is not moveable, performs a new allocation using the container allocator.
      */
      template<class U> node_ptr_type rebind_node_ptr(typename U::node_ptr_type &&p)
      {
        // If not the same allocator or not moveable, need to reallocate
        bool needToRealloc=!std::is_same<Alloc, typename U::node_ptr_type::allocator_type>::value
          || !std::is_rvalue_reference<typename U::node_ptr_type>::value;
        if(!needToRealloc)
        {
          node_ptr_type ret(_allocator);
          ret.p=p.release();
          return ret;
        }
        return node_ptr_type(_allocator, std::forward<typename U::value_type>(*p));
      }
      /*! \brief Factory function for many node_ptr_types, optionally using an array of preexisting
      memory allocations which must be deallocatable by this container's allocator.
      */
      template<class InputIterator> std::vector<node_ptr_type> make_node_ptrs(InputIterator start, InputIterator finish, value_type **to_use=nullptr)
      {
        static_assert(std::is_same<typename std::decay<typename InputIterator::value_type>::type, value_type>::value, "InputIterator type is not my value_type");
        std::vector<node_ptr_type> ret;
        // If the iterator is capable of random access, reserve the vector
        if(std::is_constructible<std::random_access_iterator_tag,
          typename std::iterator_traits<InputIterator>::iterator_category>::value)
        {
          size_type len=std::distance(start, finish);
          ret.reserve(len);
        }
        for(; start!=finish; ++start, to_use ? ++to_use : to_use)
        {
          if(to_use)
          {
            ret.push_back(node_ptr_type(_allocator));
            _allocator.construct(to_use, std::forward<typename InputIterator::value_type>(*start));
            ret.back().p=to_use;
          }
          else
            ret.push_back(node_ptr_type(_allocator, std::forward<typename InputIterator::value_type>(*start)));
        }
        return ret;
      }
    private:
      template<class C> std::pair<iterator, bool> _insert(node_ptr_type &&v, C &&extend)
      {
        std::pair<iterator, bool> ret(end(), true);
        const key_type &k=v->first;
        size_t h=_hasher(k);
        bool done=false;
        do
        {
          auto itb=_get_bucket(h);
          bucket_type &b=*itb;
          size_t emptyidx=(size_t) -1;
#if 1
          // First search for equivalents and empties.
          if(b.count.load(memory_order_acquire))
          {
            size_t offset=0;
            BOOST_BEGIN_TRANSACT_LOCK_ONLY_IF_NOT(b.lock, 2)
              for(item_type *i=b.items.data(), *e=b.items.data()+b.items.size();;)
              {
              for(; i<e && i->hash!=h; offset++, i++)
                if(emptyidx==(size_t) -1 && !i->p)
                  emptyidx=offset;
              if(i==e)
                break;
              if(i->p && _key_equal(k, i->p->first))
              {
                ret.first._itb=itb;
                ret.first._offset=offset;
                ret.second=false;
                done=true;
                break;
              }
              else offset++, i++;
              }
            BOOST_END_TRANSACT_LOCK(b.lock)
          }
          else if(!b.items.empty())
            emptyidx=0;
#endif

          if(!done)
          {
            // If the lock is other state we need to reload bucket list
            if(!b.lock.lock(2))
              continue;
            std::lock_guard<decltype(b.lock)> g(b.lock, std::adopt_lock); // Will abort all concurrency
            // If we earlier found an empty use that
            if(emptyidx!=(size_t) -1)
            {
              item_type *i=b.items.data()+emptyidx;
              if(emptyidx<b.items.size() && !i->p)
              {
                ret.first._itb=itb;
                ret.first._offset=emptyidx;
                i->p=v.release();
                i->hash=h;
                b.count.fetch_add(1, memory_order_acquire);
                done=true;
              }
            }
            if(!done)
              done=extend(ret, itb, h, std::move(v));
          }
        } while(!done);
        return ret;
      }
    public:
      std::pair<iterator, bool> insert(noalloc<node_ptr_type> &&v)
      {
        return _insert(std::move(v), [](std::pair<iterator, bool> &ret, typename std::vector<bucket_type>::iterator &itb, size_t h, node_ptr_type &&v){ return false; });
      }
      std::pair<iterator, bool> insert(node_ptr_type &&v)
      {
        return _insert(std::move(v), [](std::pair<iterator, bool> &ret, typename std::vector<bucket_type>::iterator &itb, size_t h, node_ptr_type &&v)
        {
          bucket_type &b=*itb;
          if(b.items.size()==b.items.capacity())
          {
            size_t newcapacity=b.items.capacity()*2;
            b.items.reserve(newcapacity ? newcapacity : 1);
          }
          ret.first._itb=itb;
          ret.first._offset=b.items.size();
          b.items.push_back(item_type(h, std::move(v)));
          b.count.fetch_add(1, memory_order_acquire);
          return true;
        });
      }

      template<class... Args> std::pair<iterator, bool> emplace(Args &&... args)
      {
        node_ptr_type n(make_node_ptr(std::forward<Args>(args)...));
        try
        {
          auto ret=insert(std::move(n));
          return ret;
        }
        catch(...)
        {
          // FIXME Need to restore args out of node_ptr_type.
          throw;
        }
      }
      template<class... Args> iterator emplace_hint(const_iterator position, Args &&... args) { return emplace(std::forward<Args>(args)...); }
      std::pair<iterator, bool> insert(const value_type &v) { return emplace(v); }
      std::pair<iterator, bool> insert(value_type &&v)
      {
        node_ptr_type n(make_node_ptr(std::move(v)));
        try
        {
          auto ret=insert(std::move(n));
          return ret;
        }
        catch(...)
        {
          v.~value_type();
          new(&v) value_type(std::move(*n));
          throw;
        }
      }
      iterator insert(const_iterator hint, const value_type &v) { return insert(v); }
      iterator insert(const_iterator hint, value_type &&v) { return insert(std::move(v)); }
      template<class InputIterator> void insert(InputIterator first, InputIterator last)
      {
        for(; first!=last; ++first)
          insert(*first);
      }
      void insert(std::initializer_list<value_type> i) { insert(i.begin(), i.end()); }
      node_ptr_type extract(const_iterator it)
      {
        //assert(it!=end());
        if(it==end) return node_ptr_type();
        bucket_type &b=*it._itb;
        node_ptr_type former(_allocator);
        {
          // If the lock is other state we need to reload bucket list
          if(!b.lock.lock(2))
            abort();
          std::lock_guard<decltype(b.lock)> g(b.lock, std::adopt_lock); // Will abort all concurrency
          auto &items=b.items;
          item_type *i=items.data()+it._offset;
          if(it._offset<items.size() && i->p)
          {
            former.p=i->p;
            i->p=nullptr;
            i->hash=0;
            if(it._offset==items.size()-1)
            {
              // Shrink table to minimum
              while(!items.empty() && !items.back().p)
                items.pop_back(); // Will abort all concurrency
            }
            b.count.fetch_sub(1, memory_order_acquire);
          }
        }
        return former;
      }
      node_ptr_type extract(const key_type &k)
      {
        size_t h=_hasher(k);
        bool done=false;
        node_ptr_type former(_allocator);
        do
        {
          auto itb=_get_bucket(h);
          bucket_type &b=*itb;
          size_t offset=0;
          if(b.count.load(memory_order_acquire))
          {
            // If the lock is other state we need to reload bucket list
            if(!b.lock.lock(2))
              continue;
            std::lock_guard<decltype(b.lock)> g(b.lock, std::adopt_lock); // Will abort all concurrency
            {
              auto &items=b.items;
              for(item_type *i=items.data()+offset, *e=items.data()+items.size();;)
              {
                for(; i<e && i->hash!=h; offset++, i++);
                if(i==e)
                  break;
                if(i->p && _key_equal(k, i->p->first))
                {
                  former.p=i->p;
                  i->p=nullptr;
                  i->hash=0;
                  if(offset==items.size()-1)
                  {
                    // Shrink table to minimum
                    while(!items.empty() && !items.back().p)
                      items.pop_back(); // Will abort all concurrency
                  }
                  b.count.fetch_sub(1, memory_order_acquire);
                  done=true;
                  break;
                }
                else offset++, i++;
              }
              done=true;
            }
          }
        } while(!done);
        return former;
      }
      iterator erase(const_iterator it)
      {
        iterator ret(it);
        ++ret;
        node_ptr_type e=extract(it);
        return e ? ret : end();
      }
      size_type erase(const key_type &k)
      {
        node_ptr_type e=extract(k);
        return e ? 1 : 0;
      }
      iterator erase(const_iterator first, const_iterator last)
      {
        for(; first!=last; ++first)
          erase(first);
        return last;
      }

      void clear() BOOST_NOEXCEPT
      {
        bool done;
        //std::lock_guard<decltype(_rehash_lock)> g2(_rehash_lock); // Stop other rehashes
        do
        {
          done=true;
          for(auto &b : _buckets)
          {
            // If the lock is other state we need to reload bucket list
            if(!b.lock.lock(2))
            {
              done=false;
              break;
            }
            std::lock_guard<decltype(b.lock)> g(b.lock, std::adopt_lock); // Will abort all concurrency
            for(auto &i : b.items)
            {
              node_ptr_type former(_allocator);
              former.p=i.p;
              i.p=nullptr;
            }
            b.items.clear();
            b.count.store(0, memory_order_release);
          }
        } while(!done);
      }
      void swap(concurrent_unordered_map &o)
      {
        std::lock(_rehash_lock, o._rehash_lock);
        std::lock_guard<decltype(_rehash_lock)> g1(_rehash_lock, std::adopt_lock);
        std::lock_guard<decltype(o._rehash_lock)> g2(o._rehash_lock, std::adopt_lock);
        std::swap(_hasher, o._hasher);
        std::swap(_key_equal, o._key_equal);
        _buckets.swap(o._buckets);
      }
      template<class _Hash, class _Pred, class _Alloc> void merge(concurrent_unordered_map<Key, T, _Hash, _Pred, _Alloc> &o)
      {
        typedef concurrent_unordered_map<Key, T, _Hash, _Pred, _Alloc> other_map_type;
        bool done;
        do
        {
          done=true;
          for(auto &b : o._buckets)
          {
            // If the lock is other state we need to reload bucket list
            if(!b.lock.lock(2))
            {
              done=false;
              break;
            }
            std::lock_guard<decltype(b.lock)> g(b.lock, std::adopt_lock); // Will abort all concurrency
            auto &items=b.items;
            size_t failed=0;
            for(auto &i : items)
            {
              typename other_map_type::node_ptr_type former(o._allocator);
              former.p=i.p;
              i.p=nullptr;
              node_ptr_type n(rebind_node_ptr<other_map_type>(std::move(former))); // Will reallocate only if necessary
              if(!insert(std::move(n)).second)
              {
                // Insertion failed, put it back
                // FIXME If allocators are different it could throw here during rebind, thus losing the value
                typename other_map_type::node_ptr_type former2(o.rebind_node_ptr<concurrent_unordered_map>(std::move(n)));
                i.p=former2.release();
                failed++;
              }
            }
            b.count.store(failed, memory_order_release);
            if(!failed)
              items.clear();
            else
            {
              while(!items.empty() && !items.back().p)
                items.pop_back(); // Will abort all concurrency
            }
          }
        } while(!done);
      }
      template<class U> void merge(U &o)
      {
        for(typename U::iterator it=o.begin(); it!=o.end(); it=o.begin())
        {
          typename U::node_ptr_type e=o.extract(it);
          insert(rebind_node_ptr(e));
        }
      }
      size_type bucket_count() const BOOST_NOEXCEPT{ return _buckets.size(); }
      size_type max_bucket_count() const BOOST_NOEXCEPT { return _buckets.max_size(); }
      size_type bucket_size(size_type n) const
      {
        bucket_type &b=_buckets[n];
        return b.items.count.load(memory_order_acquire);
      }
      size_type bucket(const key_type &k) const
      {
        return _hasher(k) % _buckets.size();
      }
      float load_factor() const BOOST_NOEXCEPT { return (float) size()/bucket_count(); }
      float max_load_factor() const BOOST_NOEXCEPT { return _max_load_factor; }
      void max_load_factor(float m) { _max_load_factor=m; }
    private:
      static void _reset_buckets(std::vector<bucket_type> &buckets) BOOST_NOEXCEPT
      {
        for(auto &b : buckets)
        {
          for(auto &i : b.items)
            i.p=nullptr;
          b.items.clear();
          b.count.store(0, memory_order_release);
        }
      }
    public:
      void rehash(size_type n)
      {
        std::lock_guard<decltype(_rehash_lock)> g(_rehash_lock); // Stop other rehashes
        if(n!=_buckets.size())
        {
          // Create a new buckets
          std::vector<bucket_type> tempbuckets(n);
          // Lock all new buckets
          for(auto &b : tempbuckets)
            b.lock.lock();
          // Lock all existing buckets
          for(auto &b : _buckets)
            b.lock.lock();
          // Swap old buckets with new buckets
          _buckets.swap(tempbuckets);
          // Tell all threads using old buckets to start reloading the bucket list
          for(auto &b : tempbuckets)
            b.lock.store(2);
          try
          {
            // Relocate all old bucket contents into new buckets
            for(const auto &ob : tempbuckets)
            {
              if(ob.count.load(memory_order_acquire))
              {
                for(auto &i : ob.items)
                {
                  if(i.p)
                  {
                    auto itb=_get_bucket(i.hash);
                    bucket_type &b=*itb;
                    if(b.items.size()==b.items.capacity())
                    {
                      size_t newcapacity=b.items.capacity()*2;
                      b.items.reserve(newcapacity ? newcapacity : 1);
                    }
                    b.items.push_back(item_type(i.hash, i.p));
                    b.count.fetch_add(1, memory_order_acquire);
                  }
                }
              }
            }
          }
          catch(...)
          {
            // If we saw an exception during relocation, simply restore
            // the old list which is untouched.
            _buckets.swap(tempbuckets);
            // Stop it deleting stuff
            _reset_buckets(tempbuckets);
            // Unlock buckets
            for(auto &b : _buckets)
              b.lock.unlock();
            throw;
          }
          // Stop old buckets deleting stuff
          _reset_buckets(tempbuckets);
          // Unlock buckets
          for(auto &b : _buckets)
            b.lock.unlock();
        }
      }
      void reserve(size_type n)
      {
        rehash((size_type)(n/_max_load_factor));
      }
      hasher hash_function() const { return _hasher; }
      key_equal key_eq() const { return _key_equal; }
      allocator_type get_allocator() const BOOST_NOEXCEPT { return _allocator; }
      void dump_buckets(std::ostream &s) const
      {
        for(size_t n=0; n<_buckets.size(); n++)
        {
          s << "Bucket " << n << ": size=" << _buckets[n].items.size() << " count=" << _buckets[n].count << std::endl;
        }
      }
    }; // concurrent_unordered_map
  }
}

namespace std
{
  template <class Key, class T, class Hash, class Pred, class Alloc>
  void swap(boost::spinlock::concurrent_unordered_map<Key, T, Hash, Pred, Alloc>& lhs, boost::spinlock::concurrent_unordered_map<Key, T, Hash, Pred, Alloc>& rhs)
  {
    lhs.swap(rhs);
  }
}
#endif // BOOST_SPINLOCK_HPP
